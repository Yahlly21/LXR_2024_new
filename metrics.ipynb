{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18831b05",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160299a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59163763",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, ML1M_demographic, Yahoo, Pinterest\n",
    "recommender_name = \"MLP\" ### Can be MLP, VAE, MLP_model, GMF_model, NCF\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir, DP_DIR)\n",
    "checkpoints_path = Path(export_dir, \"checkpoints\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56467a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\",\n",
    "    \"NCF\": \"single\",\n",
    "    \"MLP_model\": \"single\",\n",
    "    \"GMF_model\": \"single\"\n",
    "}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"ML1M_demographic\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155\n",
    "}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"ML1M_demographic\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362\n",
    "}\n",
    "\n",
    "demographic_dict = {\n",
    "    \"ML1M_demographic\": True,\n",
    "    \"ML1M\":False,\n",
    "    \"Yahoo\":False, \n",
    "    \"Pinterest\":False\n",
    "}\n",
    "\n",
    "features_dict = {\n",
    "    \"ML1M_demographic\": 3421,\n",
    "    \"ML1M\":None,\n",
    "    \"Yahoo\":None, \n",
    "    \"Pinterest\":None\n",
    "}\n",
    "\n",
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),\n",
    "    (\"ML1M\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_ML1M_0.0001_64_27.pt\"),\n",
    "    (\"ML1M\",\"GMF_model\"): Path(checkpoints_path, \"GMF_best_ML1M_0.0001_32_17.pt\"),\n",
    "    (\"ML1M\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_5e-05_64_16.pt\"),\n",
    "\n",
    "    (\"ML1M_demographic\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_demographic_0.0001_64_6_18.pt\"),\n",
    "    (\"ML1M_demographic\",\"MLP\"):Path(checkpoints_path, \"MLP_ML1M_demographic_0.0_64_0_28.pt\"),\n",
    "    (\"ML1M_demographic\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model2_ML1M_demographic_7e-05_128_3_22.pt\"),\n",
    "    (\"ML1M_demographic\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model_ML1M_demographic_0.00061_64_21_10.pt\"),\n",
    "    (\"ML1M_demographic\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_demographic_0.00023_32_3_2.pt\"),\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    (\"Yahoo\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_Yahoo_5e-05_32_29.pt\"),\n",
    "    (\"Yahoo\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model2_Yahoo_0.0_128_0_49.pt\"),\n",
    "    (\"Yahoo\",\"NCF\"):Path(checkpoints_path, \"NCF_Yahoo_0.001_64_21_0.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_0.0002_32_12.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    (\"Pinterest\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_Pinterest_4e-05_1024_7_18.pt\"),\n",
    "    (\"Pinterest\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model_Pinterest_0.001_512_3_19.pt\"),\n",
    "    (\"Pinterest\",\"NCF\"):Path(checkpoints_path, \"NCF2_Pinterest_9e-05_32_9_10.pt\"),\n",
    "    \n",
    "}\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "    (\"ML1M\",\"MLP_model\"): 8,\n",
    "    (\"ML1M\",\"GMF_model\"): 8,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "\n",
    "    (\"ML1M_demographic\",\"VAE\"): None,\n",
    "    (\"ML1M_demographic\",\"MLP\"): 32,\n",
    "    (\"ML1M_demographic\",\"MLP_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"GMF_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"NCF\"): 8,\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"MLP_model\"): 8,\n",
    "    (\"Yahoo\",\"GMF_model\"): 8,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"MLP_model\"): 64,\n",
    "    (\"Pinterest\",\"GMF_model\"): 64,\n",
    "    (\"Pinterest\",\"NCF\"): 64,\n",
    "}\n",
    "\n",
    "LXR_checkpoint_dict = {\n",
    "    (\"ML1M\",\"VAE\"): ('LXR_ML1M_VAE_16_19.pt',128),\n",
    "    (\"ML1M\",\"MLP\"): ('LXR_ML1M_MLP_9_20_1_7.143470844191349_0.35140294238438674.pt',128),\n",
    "    'another': 'LXR_ML1M_MLP_16_20_1_4.962869318313412_0.01958043494301176,pt',\n",
    "    (\"ML1M\",\"MLP_model\"): 8,\n",
    "    (\"ML1M\",\"GMF_model\"): 8,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "\n",
    "    (\"ML1M_demographic\",\"VAE\"): None,\n",
    "    (\"ML1M_demographic\",\"MLP\"): 32,\n",
    "    (\"ML1M_demographic\",\"MLP_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"GMF_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"NCF\"): 8,\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"MLP_model\"): 8,\n",
    "    (\"Yahoo\",\"GMF_model\"): 8,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"MLP_model\"): 64,\n",
    "    (\"Pinterest\",\"GMF_model\"): 64,\n",
    "    (\"Pinterest\",\"NCF\"): 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a2c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] \n",
    "demographic = demographic_dict[data_name]\n",
    "if demographic:\n",
    "    num_features = features_dict[data_name]\n",
    "else:\n",
    "    num_features = num_items_dict[data_name]\n",
    "hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "\n",
    "recommender_path = recommender_path_dict[(data_name,recommender_name)]\n",
    "lxr_path = LXR_checkpoint_dict[(data_name,recommender_name)][0]\n",
    "lxr_dim = LXR_checkpoint_dict[(data_name,recommender_name)][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6b87",
   "metadata": {},
   "source": [
    "## Data and baselines imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1b88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29108508",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = static_test_data.iloc[:,:-2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f572bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'jaccard_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    jaccard_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1b5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'cosine_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    cosine_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630bf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'tf_idf_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    tf_idf_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b741d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_features):\n",
    "    for j in range(i, num_features):\n",
    "        jaccard_dict[(j,i)]= jaccard_dict[(i,j)]\n",
    "        cosine_dict[(j,i)]= cosine_dict[(i,j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a826d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df5e9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "          'demographic':demographic,\n",
    "          'num_features':num_features,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db85e",
   "metadata": {},
   "source": [
    "# Recommenders Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09da5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "\n",
    "VAE_config= {\n",
    "\"enc_dims\": [512,128],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63243da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name=='MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='VAE':\n",
    "        recommender = VAE(VAE_config, **kw_dict)\n",
    "    elif recommender_name=='MLP_model':\n",
    "        recommender = MLP_model(hidden_size=hidden_dim, num_layers=3, **kw_dict)\n",
    "    elif recommender_name=='GMF_model':\n",
    "        recommender = GMF_model(hidden_size=hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='NCF':\n",
    "        MLP_temp = MLP_model(hidden_size=hidden_dim, num_layers=3, **kw_dict)\n",
    "        GMF_temp = GMF_model(hidden_size=hidden_dim, **kw_dict)\n",
    "        recommender = NCF(factor_num=hidden_dim, num_layers=3, dropout=0.5, model= 'NeuMF-pre', GMF_model= GMF_temp, MLP_model=MLP_temp, **kw_dict)\n",
    "    recommender_checkpoint = torch.load(Path(checkpoints_path, recommender_path))\n",
    "    recommender.load_state_dict(recommender_checkpoint)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad= False\n",
    "    return recommender\n",
    "    \n",
    "recommender = load_recommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94540246",
   "metadata": {},
   "source": [
    "# LXR definition and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48225b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(nn.Module):\n",
    "    def __init__(self, user_size, item_size, hidden_size):\n",
    "        super(Explainer, self).__init__()\n",
    "        \n",
    "        self.users_fc = nn.Linear(in_features = user_size, out_features=hidden_size).to(device)\n",
    "        self.items_fc = nn.Linear(in_features = item_size, out_features=hidden_size).to(device)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size).to(device),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size, out_features=user_size).to(device),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_output = self.users_fc(user_tensor.float())\n",
    "        item_output = self.items_fc(item_tensor.float())\n",
    "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
    "        expl_scores = self.bottleneck(combined_output).to(device)\n",
    "        return expl_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97c7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(num_features, num_items, lxr_dim)\n",
    "lxr_checkpoint = torch.load(Path(checkpoints_path, lxr_path))\n",
    "explainer.load_state_dict(lxr_checkpoint)\n",
    "explainer.eval()\n",
    "for param in explainer.parameters():\n",
    "    param.requires_grad= False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc79dc",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72dd2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96048f75",
   "metadata": {},
   "source": [
    "# Baselines functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "097338a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, get_lire_args\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, get_lire_args\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25797d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_mask(x, item_id, user_based_Jaccard_sim):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in user_based_Jaccard_sim:\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0            \n",
    "\n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "950130f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, item_cosine):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in item_cosine:\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)]\n",
    "            else:\n",
    "                item_cosine_dict[i]=0\n",
    "\n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "baff2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity mask\n",
    "def find_pop_mask(x, item_id):\n",
    "    user_hist = torch.Tensor(x).to(device) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    \n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            item_pop_dict[i]=pop_array[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e9fb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lime_mask(x, item_id, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, recommender, num_samples=10, method = 'POS', **kw_dict):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lime_args(user_hist, item_id, recommender, all_items_tensor, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id, **kw_dict)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff3e5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lire_mask(x, item_id, num_of_perturbations, kernel_func, feature_selection, recommender, proba=0.1, method = 'POS', **kw_dict):\n",
    "    \n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "\n",
    "    \n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lire_args(user_hist, item_id, recommender, all_items_tensor, train_array, num_of_perturbations = num_of_perturbations, seed = item_id, proba=0.1, **kw_dict)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_of_perturbations, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_of_perturbations, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dda1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf_idf_mask(x, item_id, tf_idf_sim, user_id):\n",
    "\n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "    user_tf_idf_scores = tf_idf_sim[user_id].copy()\n",
    "  \n",
    "    return user_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f33bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fia_mask(user_tensor, item_tensor, item_id, recommender):\n",
    "    y_pred = recommender_run(user_tensor, recommender, item_tensor, item_id, **kw_dict).to(device)\n",
    "    items_fia = {}\n",
    "    user_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        if(user_hist[i] == 1):\n",
    "            user_hist[i] = 0\n",
    "            user_tensor = torch.FloatTensor(user_hist).to(device)\n",
    "            y_pred_without_item = recommender_run(user_tensor, recommender, item_tensor, item_id, 'single', **kw_dict).to(device)\n",
    "            infl_score = y_pred - y_pred_without_item\n",
    "            items_fia[i] = infl_score\n",
    "            user_hist[i] = 1\n",
    "\n",
    "    return items_fia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69a35c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, top_k):\n",
    "   \n",
    "    items_accent = defaultdict(float)\n",
    "    factor = top_k - 1\n",
    "    user_accent_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "\n",
    "    #Get topk items\n",
    "    sorted_indices = list(get_top_k(user_tensor, user_tensor, recommender_model, **kw_dict).keys())\n",
    "    \n",
    "    if top_k == 1:\n",
    "        # When k=1, return the index of the first maximum value\n",
    "        top_k_indices = [sorted_indices[0]]\n",
    "    else:\n",
    "        top_k_indices = sorted_indices[:top_k]\n",
    "   \n",
    "\n",
    "    for iteration, item_k_id in enumerate(top_k_indices):\n",
    "\n",
    "        # Set topk items to 0 in the user's history\n",
    "        user_accent_hist[item_k_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_accent_hist).to(device)\n",
    "       \n",
    "        item_vector = items_array[item_k_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "              \n",
    "        # Check influence of the items in the history on this specific item in topk\n",
    "        fia_dict = find_fia_mask(user_tensor, item_tensor, item_k_id, recommender_model)\n",
    "         \n",
    "        # Sum up all differences between influence on top1 and other topk values\n",
    "        if not iteration:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] *= factor\n",
    "        else:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] -= fia_dict[key]\n",
    "       \n",
    "    for key in items_accent.keys():\n",
    "        items_accent[key] *= -1    \n",
    "\n",
    "    return items_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "181c5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lxr_mask(x, item_tensor):\n",
    "    \n",
    "    user_hist = x\n",
    "    mask = explainer(user_hist, item_tensor)\n",
    "    x_masked = user_hist*mask\n",
    "    item_sim_dict = {}\n",
    "    for i,j in enumerate(x_masked>0):\n",
    "        if j:\n",
    "            item_sim_dict[i]=x_masked[i] \n",
    "        \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb094e",
   "metadata": {},
   "source": [
    "# Evaluation help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad5cdcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender_model, user_id = None, mask_type = None):\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    if mask_type == 'lime':\n",
    "        POS_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity,'highest_weights', recommender_model, num_samples=user_hist_size, **kw_dict)\n",
    "        NEG_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity,'highest_weights', recommender_model, num_samples=user_hist_size, method = 'NEG', **kw_dict)\n",
    "    elif mask_type == 'lire':\n",
    "        POS_sim_items = find_lire_mask(user_vector, item_id, 200, distance_to_proximity,'highest_weights', recommender_model,proba = 0.1, **kw_dict)\n",
    "        NEG_sim_items = find_lire_mask(user_vector, item_id, 200, distance_to_proximity,'highest_weights', recommender_model,proba = 0.1, method = 'NEG', **kw_dict)\n",
    "    else:\n",
    "        if mask_type == 'jaccard':\n",
    "            sim_items = find_jaccard_mask(user_tensor, item_id, jaccard_dict)\n",
    "        elif mask_type == 'cosine':\n",
    "            sim_items = find_cosine_mask(user_tensor, item_id, cosine_dict)\n",
    "        elif mask_type == 'pop':\n",
    "            sim_items = find_pop_mask(user_tensor, item_id)\n",
    "        elif mask_type == 'tf_idf':\n",
    "            sim_items = find_tf_idf_mask(user_tensor, item_id, tf_idf_dict, user_id)\n",
    "        elif mask_type == 'shap':\n",
    "            sim_items = find_shapley_mask(user_tensor, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "        elif mask_type == 'fia':\n",
    "            sim_items = find_fia_mask(user_tensor, item_tensor, item_id, recommender_model) \n",
    "        elif mask_type == 'accent':\n",
    "            sim_items = find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, 5)\n",
    "        elif mask_type == 'lxr':\n",
    "            sim_items = find_lxr_mask(user_tensor, item_tensor)\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        \n",
    "    return POS_sim_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9be55f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender_model, expl_dict, **kw_dict):\n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "    \n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    rankA_at_1 = [0]*(len(bins))\n",
    "    rankA_at_5 = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    rankB = [0]*(len(bins))\n",
    "    NDCG = [0]*(len(bins))\n",
    "\n",
    "    \n",
    "    POS_sim_items = expl_dict\n",
    "    NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "    \n",
    "    total_items=0\n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "            \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked # remove the masked items from the user history\n",
    "\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "        \n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor, recommender_model, **kw_dict)\n",
    "        \n",
    "        if item_id in list(POS_ranked_list.keys()):\n",
    "            POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        else:\n",
    "            POS_index = num_items\n",
    "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, recommender_model, **kw_dict)+1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_run(POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_run(user_tensor-POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1+1-POS_index)/1)\n",
    "        rankA_at_5[i] = max(0, (5+1-POS_index)/5)\n",
    "        rankA_at_10[i] = max(0, (10+1-POS_index)/10)\n",
    "        rankA_at_20[i] = max(0, (20+1-POS_index)/20)\n",
    "        rankA_at_50[i] = max(0, (50+1-POS_index)/50)\n",
    "        rankA_at_100[i] = max(0, (100+1-POS_index)/100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1/POS_index\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG[i]= get_ndcg(list(POS_ranked_list.keys()),item_id, **kw_dict)\n",
    "        \n",
    "    res = [DEL, INS, rankB, NDCG, POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100,  NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100,  rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4132628b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "User 0, total time: 1.80\n",
      "User 1, total time: 1.15\n",
      "User 2, total time: 1.02\n",
      "User 3, total time: 1.06\n",
      "User 4, total time: 1.56\n",
      "User 5, total time: 1.14\n",
      "User 6, total time: 1.11\n",
      "User 7, total time: 1.11\n",
      "User 8, total time: 1.17\n",
      "User 9, total time: 1.15\n",
      "User 10, total time: 1.11\n",
      "User 11, total time: 1.14\n",
      "User 12, total time: 1.41\n",
      "User 13, total time: 1.41\n",
      "User 14, total time: 1.04\n",
      "User 15, total time: 1.00\n",
      "User 16, total time: 1.26\n",
      "User 17, total time: 1.15\n",
      "User 18, total time: 1.01\n",
      "User 19, total time: 1.02\n",
      "User 20, total time: 1.05\n",
      "User 21, total time: 1.20\n",
      "User 22, total time: 1.44\n",
      "User 23, total time: 1.04\n",
      "User 24, total time: 1.23\n",
      "User 25, total time: 1.14\n",
      "User 26, total time: 1.32\n",
      "User 27, total time: 1.11\n",
      "User 28, total time: 1.59\n",
      "User 29, total time: 1.02\n",
      "User 30, total time: 1.09\n",
      "User 31, total time: 1.27\n",
      "User 32, total time: 1.05\n",
      "User 33, total time: 1.03\n",
      "User 34, total time: 1.51\n",
      "User 35, total time: 1.21\n",
      "User 36, total time: 1.04\n",
      "User 37, total time: 1.66\n",
      "User 38, total time: 1.12\n",
      "User 39, total time: 1.16\n",
      "User 40, total time: 1.41\n",
      "User 41, total time: 1.19\n",
      "User 42, total time: 1.73\n",
      "User 43, total time: 1.21\n",
      "User 44, total time: 1.05\n",
      "User 45, total time: 1.14\n",
      "User 46, total time: 2.21\n",
      "User 47, total time: 1.31\n",
      "User 48, total time: 1.04\n",
      "User 49, total time: 1.41\n",
      "User 50, total time: 1.10\n",
      "User 51, total time: 1.22\n",
      "User 52, total time: 1.11\n",
      "User 53, total time: 1.25\n",
      "User 54, total time: 1.16\n",
      "User 55, total time: 1.20\n",
      "User 56, total time: 1.55\n",
      "User 57, total time: 1.10\n",
      "User 58, total time: 1.08\n",
      "User 59, total time: 1.29\n",
      "User 60, total time: 1.11\n",
      "User 61, total time: 1.25\n",
      "User 62, total time: 1.29\n",
      "User 63, total time: 1.38\n",
      "User 64, total time: 1.04\n",
      "User 65, total time: 1.24\n",
      "User 66, total time: 1.25\n",
      "User 67, total time: 1.74\n",
      "User 68, total time: 1.26\n",
      "User 69, total time: 1.19\n",
      "User 70, total time: 1.13\n",
      "User 71, total time: 1.12\n",
      "User 72, total time: 1.06\n",
      "User 73, total time: 1.13\n",
      "User 74, total time: 1.06\n",
      "User 75, total time: 1.17\n",
      "User 76, total time: 1.05\n",
      "User 77, total time: 1.25\n",
      "User 78, total time: 1.20\n",
      "User 79, total time: 1.19\n",
      "User 80, total time: 1.63\n",
      "User 81, total time: 1.21\n",
      "User 82, total time: 1.12\n",
      "User 83, total time: 1.17\n",
      "User 84, total time: 1.00\n",
      "User 85, total time: 1.33\n",
      "User 86, total time: 1.04\n",
      "User 87, total time: 1.06\n",
      "User 88, total time: 1.22\n",
      "User 89, total time: 1.04\n",
      "User 90, total time: 1.04\n",
      "User 91, total time: 1.59\n",
      "User 92, total time: 1.44\n",
      "User 93, total time: 1.46\n",
      "User 94, total time: 1.10\n",
      "User 95, total time: 1.12\n",
      "User 96, total time: 1.45\n",
      "User 97, total time: 1.30\n",
      "User 98, total time: 1.21\n",
      "User 99, total time: 1.12\n",
      "User 100, total time: 1.05\n",
      "User 101, total time: 1.34\n",
      "User 102, total time: 1.08\n",
      "User 103, total time: 1.13\n",
      "User 104, total time: 1.20\n",
      "User 105, total time: 1.15\n",
      "User 106, total time: 1.05\n",
      "User 107, total time: 1.36\n",
      "User 108, total time: 1.80\n",
      "User 109, total time: 1.14\n",
      "User 110, total time: 1.26\n",
      "User 111, total time: 1.06\n",
      "User 112, total time: 1.25\n",
      "User 113, total time: 1.40\n",
      "User 114, total time: 1.26\n",
      "User 115, total time: 1.24\n",
      "User 116, total time: 1.11\n",
      "User 117, total time: 1.12\n",
      "User 118, total time: 1.11\n",
      "User 119, total time: 1.09\n",
      "User 120, total time: 1.08\n",
      "User 121, total time: 1.08\n",
      "User 122, total time: 1.10\n",
      "User 123, total time: 1.16\n",
      "User 124, total time: 1.14\n",
      "User 125, total time: 1.19\n",
      "User 126, total time: 1.00\n",
      "User 127, total time: 1.62\n",
      "User 128, total time: 1.14\n",
      "User 129, total time: 1.12\n",
      "User 130, total time: 1.29\n",
      "User 131, total time: 1.03\n",
      "User 132, total time: 1.21\n",
      "User 133, total time: 1.07\n",
      "User 134, total time: 1.21\n",
      "User 135, total time: 1.02\n",
      "User 136, total time: 1.02\n",
      "User 137, total time: 1.04\n",
      "User 138, total time: 1.07\n",
      "User 139, total time: 1.03\n",
      "User 140, total time: 1.34\n",
      "User 141, total time: 1.11\n",
      "User 142, total time: 1.12\n",
      "User 143, total time: 1.16\n",
      "User 144, total time: 1.20\n",
      "User 145, total time: 1.74\n",
      "User 146, total time: 1.10\n",
      "User 147, total time: 1.05\n",
      "User 148, total time: 1.14\n",
      "User 149, total time: 1.11\n",
      "User 150, total time: 1.26\n",
      "User 151, total time: 1.62\n",
      "User 152, total time: 1.78\n",
      "User 153, total time: 1.18\n",
      "User 154, total time: 1.08\n",
      "User 155, total time: 1.36\n",
      "User 156, total time: 1.42\n",
      "User 157, total time: 1.19\n",
      "User 158, total time: 1.22\n",
      "User 159, total time: 1.27\n",
      "User 160, total time: 1.16\n",
      "User 161, total time: 1.09\n",
      "User 162, total time: 1.50\n",
      "User 163, total time: 1.36\n",
      "User 164, total time: 1.12\n",
      "User 165, total time: 1.15\n",
      "User 166, total time: 1.05\n",
      "User 167, total time: 1.10\n",
      "User 168, total time: 1.12\n",
      "User 169, total time: 1.15\n",
      "User 170, total time: 1.10\n",
      "User 171, total time: 1.12\n",
      "User 172, total time: 1.07\n",
      "User 173, total time: 1.16\n",
      "User 174, total time: 1.10\n",
      "User 175, total time: 1.04\n",
      "User 176, total time: 1.13\n",
      "User 177, total time: 1.30\n",
      "User 178, total time: 1.34\n",
      "User 179, total time: 1.38\n",
      "User 180, total time: 1.34\n",
      "User 181, total time: 1.16\n",
      "User 182, total time: 1.46\n",
      "User 183, total time: 1.23\n",
      "User 184, total time: 1.12\n",
      "User 185, total time: 1.07\n",
      "User 186, total time: 1.15\n",
      "User 187, total time: 1.10\n",
      "User 188, total time: 1.17\n",
      "User 189, total time: 1.15\n",
      "User 190, total time: 1.09\n",
      "User 191, total time: 1.56\n",
      "User 192, total time: 1.24\n",
      "User 193, total time: 1.34\n",
      "User 194, total time: 1.11\n",
      "User 195, total time: 1.06\n",
      "User 196, total time: 1.15\n",
      "User 197, total time: 1.22\n",
      "User 198, total time: 1.06\n",
      "User 199, total time: 1.09\n",
      "User 200, total time: 1.25\n",
      "User 201, total time: 1.08\n",
      "User 202, total time: 1.43\n",
      "User 203, total time: 1.70\n",
      "User 204, total time: 1.15\n",
      "User 205, total time: 1.70\n",
      "User 206, total time: 1.20\n",
      "User 207, total time: 1.09\n",
      "User 208, total time: 1.15\n",
      "User 209, total time: 1.05\n",
      "User 210, total time: 1.02\n",
      "User 211, total time: 1.48\n",
      "User 212, total time: 1.00\n",
      "User 213, total time: 1.22\n",
      "User 214, total time: 1.04\n",
      "User 215, total time: 1.09\n",
      "User 216, total time: 1.35\n",
      "User 217, total time: 1.01\n",
      "User 218, total time: 1.10\n",
      "User 219, total time: 1.43\n",
      "User 220, total time: 1.01\n",
      "User 221, total time: 1.12\n",
      "User 222, total time: 1.03\n",
      "User 223, total time: 1.03\n",
      "User 224, total time: 1.05\n",
      "User 225, total time: 1.02\n",
      "User 226, total time: 1.08\n",
      "User 227, total time: 1.28\n",
      "User 228, total time: 1.03\n",
      "User 229, total time: 1.02\n",
      "User 230, total time: 1.03\n",
      "User 231, total time: 1.00\n",
      "User 232, total time: 1.05\n",
      "User 233, total time: 1.68\n",
      "User 234, total time: 1.28\n",
      "User 235, total time: 1.30\n",
      "User 236, total time: 0.99\n",
      "User 237, total time: 1.04\n",
      "User 238, total time: 1.12\n",
      "User 239, total time: 1.04\n",
      "User 240, total time: 1.14\n",
      "User 241, total time: 1.07\n",
      "User 242, total time: 1.17\n",
      "User 243, total time: 1.40\n",
      "User 244, total time: 1.13\n",
      "User 245, total time: 1.02\n",
      "User 246, total time: 1.03\n",
      "User 247, total time: 1.00\n",
      "User 248, total time: 1.03\n",
      "User 249, total time: 1.11\n",
      "User 250, total time: 1.08\n",
      "User 251, total time: 1.80\n",
      "User 252, total time: 1.06\n",
      "User 253, total time: 1.16\n",
      "User 254, total time: 1.30\n",
      "User 255, total time: 1.02\n",
      "User 256, total time: 1.07\n",
      "User 257, total time: 1.06\n",
      "User 258, total time: 1.23\n",
      "User 259, total time: 1.83\n",
      "User 260, total time: 1.12\n",
      "User 261, total time: 1.09\n",
      "User 262, total time: 1.16\n",
      "User 263, total time: 1.09\n",
      "User 264, total time: 1.26\n",
      "User 265, total time: 1.45\n",
      "User 266, total time: 1.25\n",
      "User 267, total time: 1.45\n",
      "User 268, total time: 1.10\n",
      "User 269, total time: 1.04\n",
      "User 270, total time: 1.87\n",
      "User 271, total time: 1.12\n",
      "User 272, total time: 1.05\n",
      "User 273, total time: 1.35\n",
      "User 274, total time: 1.11\n",
      "User 275, total time: 1.12\n",
      "User 276, total time: 1.06\n",
      "User 277, total time: 1.08\n",
      "User 278, total time: 1.08\n",
      "User 279, total time: 1.59\n",
      "User 280, total time: 1.12\n",
      "User 281, total time: 0.99\n",
      "User 282, total time: 1.12\n",
      "User 283, total time: 1.14\n",
      "User 284, total time: 1.05\n",
      "User 285, total time: 1.07\n",
      "User 286, total time: 1.30\n",
      "User 287, total time: 1.01\n",
      "User 288, total time: 1.21\n",
      "User 289, total time: 1.07\n",
      "User 290, total time: 1.06\n",
      "User 291, total time: 1.09\n",
      "User 292, total time: 1.93\n",
      "User 293, total time: 1.30\n",
      "User 294, total time: 1.36\n",
      "User 295, total time: 1.33\n",
      "User 296, total time: 1.20\n",
      "User 297, total time: 1.13\n",
      "User 298, total time: 1.11\n",
      "User 299, total time: 1.10\n",
      "User 300, total time: 1.33\n",
      "User 301, total time: 1.21\n",
      "User 302, total time: 1.22\n",
      "User 303, total time: 2.07\n",
      "User 304, total time: 1.14\n",
      "User 305, total time: 1.03\n",
      "User 306, total time: 1.35\n",
      "User 307, total time: 1.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 308, total time: 1.16\n",
      "User 309, total time: 1.14\n",
      "User 310, total time: 1.04\n",
      "User 311, total time: 1.12\n",
      "User 312, total time: 1.66\n",
      "User 313, total time: 1.12\n",
      "User 314, total time: 1.00\n",
      "User 315, total time: 1.06\n",
      "User 316, total time: 1.07\n",
      "User 317, total time: 1.61\n",
      "User 318, total time: 1.25\n",
      "User 319, total time: 1.09\n",
      "User 320, total time: 1.03\n",
      "User 321, total time: 1.06\n",
      "User 322, total time: 1.00\n",
      "User 323, total time: 1.45\n",
      "User 324, total time: 1.00\n",
      "User 325, total time: 1.13\n",
      "User 326, total time: 1.04\n",
      "User 327, total time: 0.94\n",
      "User 328, total time: 1.05\n",
      "User 329, total time: 0.98\n",
      "User 330, total time: 1.39\n",
      "User 331, total time: 1.17\n",
      "User 332, total time: 1.17\n",
      "User 333, total time: 1.03\n",
      "User 334, total time: 1.26\n",
      "User 335, total time: 1.12\n",
      "User 336, total time: 1.01\n",
      "User 337, total time: 1.27\n",
      "User 338, total time: 1.00\n",
      "User 339, total time: 1.15\n",
      "User 340, total time: 1.20\n",
      "User 341, total time: 1.16\n",
      "User 342, total time: 1.15\n",
      "User 343, total time: 1.08\n",
      "User 344, total time: 1.12\n",
      "User 345, total time: 1.63\n",
      "User 346, total time: 1.64\n",
      "User 347, total time: 1.41\n",
      "User 348, total time: 1.11\n",
      "User 349, total time: 1.45\n",
      "User 350, total time: 1.23\n",
      "User 351, total time: 1.63\n",
      "User 352, total time: 1.10\n",
      "User 353, total time: 1.15\n",
      "User 354, total time: 1.09\n",
      "User 355, total time: 1.08\n",
      "User 356, total time: 1.17\n",
      "User 357, total time: 1.14\n",
      "User 358, total time: 1.71\n",
      "User 359, total time: 1.11\n",
      "User 360, total time: 1.77\n",
      "User 361, total time: 1.21\n",
      "User 362, total time: 1.37\n",
      "User 363, total time: 1.09\n",
      "User 364, total time: 1.10\n",
      "User 365, total time: 1.06\n",
      "User 366, total time: 1.02\n",
      "User 367, total time: 1.04\n",
      "User 368, total time: 1.03\n",
      "User 369, total time: 1.30\n",
      "User 370, total time: 1.11\n",
      "User 371, total time: 1.12\n",
      "User 372, total time: 1.14\n",
      "User 373, total time: 1.07\n",
      "User 374, total time: 1.16\n",
      "User 375, total time: 1.30\n",
      "User 376, total time: 1.21\n",
      "User 377, total time: 1.06\n",
      "User 378, total time: 1.01\n",
      "User 379, total time: 1.16\n",
      "User 380, total time: 1.33\n",
      "User 381, total time: 1.45\n",
      "User 382, total time: 1.55\n",
      "User 383, total time: 1.17\n",
      "User 384, total time: 1.09\n",
      "User 385, total time: 1.05\n",
      "User 386, total time: 1.15\n",
      "User 387, total time: 1.29\n",
      "User 388, total time: 1.13\n",
      "User 389, total time: 1.06\n",
      "User 390, total time: 1.07\n",
      "User 391, total time: 1.07\n",
      "User 392, total time: 1.04\n",
      "User 393, total time: 1.14\n",
      "User 394, total time: 1.06\n",
      "User 395, total time: 1.16\n",
      "User 396, total time: 1.22\n",
      "User 397, total time: 1.36\n",
      "User 398, total time: 1.53\n",
      "User 399, total time: 1.20\n",
      "User 400, total time: 1.16\n",
      "User 401, total time: 1.08\n",
      "User 402, total time: 1.28\n",
      "User 403, total time: 1.00\n",
      "User 404, total time: 1.20\n",
      "User 405, total time: 1.04\n",
      "User 406, total time: 0.99\n",
      "User 407, total time: 1.88\n",
      "User 408, total time: 1.10\n",
      "User 409, total time: 1.29\n",
      "User 410, total time: 1.21\n",
      "User 411, total time: 1.06\n",
      "User 412, total time: 1.26\n",
      "User 413, total time: 1.20\n",
      "User 414, total time: 1.16\n",
      "User 415, total time: 1.04\n",
      "User 416, total time: 1.33\n",
      "User 417, total time: 1.01\n",
      "User 418, total time: 1.03\n",
      "User 419, total time: 1.43\n",
      "User 420, total time: 1.48\n",
      "User 421, total time: 1.46\n",
      "User 422, total time: 1.04\n",
      "User 423, total time: 1.25\n",
      "User 424, total time: 1.03\n",
      "User 425, total time: 1.07\n",
      "User 426, total time: 1.13\n",
      "User 427, total time: 1.04\n",
      "User 428, total time: 1.11\n",
      "User 429, total time: 1.01\n",
      "User 430, total time: 1.75\n",
      "User 431, total time: 0.99\n",
      "User 432, total time: 1.20\n",
      "User 433, total time: 1.11\n",
      "User 434, total time: 1.07\n",
      "User 435, total time: 1.00\n",
      "User 436, total time: 1.08\n",
      "User 437, total time: 0.99\n",
      "User 438, total time: 1.12\n",
      "User 439, total time: 1.00\n",
      "User 440, total time: 1.19\n",
      "User 441, total time: 1.44\n",
      "User 442, total time: 1.20\n",
      "User 443, total time: 1.11\n",
      "User 444, total time: 1.10\n",
      "User 445, total time: 1.33\n",
      "User 446, total time: 1.11\n",
      "User 447, total time: 1.06\n",
      "User 448, total time: 1.02\n",
      "User 449, total time: 1.19\n",
      "User 450, total time: 1.39\n",
      "User 451, total time: 1.43\n",
      "User 452, total time: 1.05\n",
      "User 453, total time: 2.15\n",
      "User 454, total time: 1.14\n",
      "User 455, total time: 1.13\n",
      "User 456, total time: 1.07\n",
      "User 457, total time: 1.19\n",
      "User 458, total time: 1.35\n",
      "User 459, total time: 1.27\n",
      "User 460, total time: 1.24\n",
      "User 461, total time: 1.17\n",
      "User 462, total time: 1.52\n",
      "User 463, total time: 1.14\n",
      "User 464, total time: 1.19\n",
      "User 465, total time: 1.02\n",
      "User 466, total time: 1.05\n",
      "User 467, total time: 1.28\n",
      "User 468, total time: 1.02\n",
      "User 469, total time: 1.28\n",
      "User 470, total time: 1.21\n",
      "User 471, total time: 1.02\n",
      "User 472, total time: 1.16\n",
      "User 473, total time: 1.03\n",
      "User 474, total time: 1.05\n",
      "User 475, total time: 1.05\n",
      "User 476, total time: 1.08\n",
      "User 477, total time: 1.11\n",
      "User 478, total time: 1.14\n",
      "User 479, total time: 1.52\n",
      "User 480, total time: 1.11\n",
      "User 481, total time: 1.11\n",
      "User 482, total time: 1.22\n",
      "User 483, total time: 1.24\n",
      "User 484, total time: 1.27\n",
      "User 485, total time: 1.09\n",
      "User 486, total time: 1.27\n",
      "User 487, total time: 1.61\n",
      "User 488, total time: 1.17\n",
      "User 489, total time: 1.15\n",
      "User 490, total time: 1.19\n",
      "User 491, total time: 1.04\n",
      "User 492, total time: 1.05\n",
      "User 493, total time: 1.07\n",
      "User 494, total time: 1.30\n",
      "User 495, total time: 1.16\n",
      "User 496, total time: 1.03\n",
      "User 497, total time: 1.15\n",
      "User 498, total time: 1.15\n",
      "User 499, total time: 1.52\n",
      "500\n",
      "User 500, total time: 1.22\n",
      "User 501, total time: 1.30\n",
      "User 502, total time: 1.14\n",
      "User 503, total time: 1.10\n",
      "User 504, total time: 1.42\n",
      "User 505, total time: 1.08\n",
      "User 506, total time: 1.10\n",
      "User 507, total time: 1.18\n",
      "User 508, total time: 1.28\n",
      "User 509, total time: 1.09\n",
      "User 510, total time: 1.93\n",
      "User 511, total time: 1.05\n",
      "User 512, total time: 1.01\n",
      "User 513, total time: 1.08\n",
      "User 514, total time: 1.17\n",
      "User 515, total time: 1.16\n",
      "User 516, total time: 1.71\n",
      "User 517, total time: 1.05\n",
      "User 518, total time: 1.12\n",
      "User 519, total time: 1.11\n",
      "User 520, total time: 1.27\n",
      "User 521, total time: 1.23\n",
      "User 522, total time: 1.30\n",
      "User 523, total time: 1.08\n",
      "User 524, total time: 1.30\n",
      "User 525, total time: 1.11\n",
      "User 526, total time: 1.04\n",
      "User 527, total time: 1.08\n",
      "User 528, total time: 1.31\n",
      "User 529, total time: 1.14\n",
      "User 530, total time: 1.14\n",
      "User 531, total time: 1.05\n",
      "User 532, total time: 1.16\n",
      "User 533, total time: 1.27\n",
      "User 534, total time: 1.41\n",
      "User 535, total time: 1.14\n",
      "User 536, total time: 1.35\n",
      "User 537, total time: 2.60\n",
      "User 538, total time: 1.08\n",
      "User 539, total time: 1.04\n",
      "User 540, total time: 1.69\n",
      "User 541, total time: 1.36\n",
      "User 542, total time: 1.30\n",
      "User 543, total time: 1.14\n",
      "User 544, total time: 1.00\n",
      "User 545, total time: 1.23\n",
      "User 546, total time: 1.25\n",
      "User 547, total time: 1.11\n",
      "User 548, total time: 1.21\n",
      "User 549, total time: 1.22\n",
      "User 550, total time: 1.17\n",
      "User 551, total time: 1.11\n",
      "User 552, total time: 1.30\n",
      "User 553, total time: 1.28\n",
      "User 554, total time: 1.35\n",
      "User 555, total time: 1.22\n",
      "User 556, total time: 1.49\n",
      "User 557, total time: 1.60\n",
      "User 558, total time: 1.19\n",
      "User 559, total time: 1.11\n",
      "User 560, total time: 1.14\n",
      "User 561, total time: 1.10\n",
      "User 562, total time: 1.36\n",
      "User 563, total time: 1.85\n",
      "User 564, total time: 1.22\n",
      "User 565, total time: 1.52\n",
      "User 566, total time: 1.22\n",
      "User 567, total time: 1.16\n",
      "User 568, total time: 1.14\n",
      "User 569, total time: 1.11\n",
      "User 570, total time: 1.28\n",
      "User 571, total time: 1.07\n",
      "User 572, total time: 1.20\n",
      "User 573, total time: 1.18\n",
      "User 574, total time: 1.04\n",
      "User 575, total time: 1.05\n",
      "User 576, total time: 1.06\n",
      "User 577, total time: 1.05\n",
      "User 578, total time: 1.11\n",
      "User 579, total time: 1.11\n",
      "User 580, total time: 1.39\n",
      "User 581, total time: 1.24\n",
      "User 582, total time: 1.16\n",
      "User 583, total time: 1.19\n",
      "User 584, total time: 1.11\n",
      "User 585, total time: 1.18\n",
      "User 586, total time: 1.19\n",
      "User 587, total time: 1.08\n",
      "User 588, total time: 1.12\n",
      "User 589, total time: 1.15\n",
      "User 590, total time: 1.16\n",
      "User 591, total time: 1.44\n",
      "User 592, total time: 1.43\n",
      "User 593, total time: 1.19\n",
      "User 594, total time: 1.29\n",
      "User 595, total time: 1.08\n",
      "User 596, total time: 1.10\n",
      "User 597, total time: 1.42\n",
      "User 598, total time: 1.25\n",
      "User 599, total time: 1.29\n",
      "User 600, total time: 1.07\n",
      "User 601, total time: 1.09\n",
      "User 602, total time: 1.05\n",
      "User 603, total time: 1.06\n",
      "User 604, total time: 1.24\n",
      "User 605, total time: 1.08\n",
      "User 606, total time: 1.19\n",
      "User 607, total time: 1.19\n",
      "User 608, total time: 1.04\n",
      "User 609, total time: 1.41\n",
      "User 610, total time: 1.06\n",
      "User 611, total time: 1.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 612, total time: 1.09\n",
      "User 613, total time: 1.15\n",
      "User 614, total time: 1.21\n",
      "User 615, total time: 1.02\n",
      "User 616, total time: 1.72\n",
      "User 617, total time: 1.04\n",
      "User 618, total time: 1.12\n",
      "User 619, total time: 1.20\n",
      "User 620, total time: 1.03\n",
      "User 621, total time: 1.15\n",
      "User 622, total time: 1.05\n",
      "User 623, total time: 1.01\n",
      "User 624, total time: 1.50\n",
      "User 625, total time: 1.05\n",
      "User 626, total time: 1.43\n",
      "User 627, total time: 1.62\n",
      "User 628, total time: 1.04\n",
      "User 629, total time: 1.39\n",
      "User 630, total time: 1.12\n",
      "User 631, total time: 1.09\n",
      "User 632, total time: 1.22\n",
      "User 633, total time: 1.07\n",
      "User 634, total time: 1.07\n",
      "User 635, total time: 1.53\n",
      "User 636, total time: 1.10\n",
      "User 637, total time: 1.01\n",
      "User 638, total time: 1.57\n",
      "User 639, total time: 1.09\n",
      "User 640, total time: 1.42\n",
      "User 641, total time: 1.15\n",
      "User 642, total time: 0.99\n",
      "User 643, total time: 1.88\n",
      "User 644, total time: 1.21\n",
      "User 645, total time: 1.13\n",
      "User 646, total time: 1.02\n",
      "User 647, total time: 1.16\n",
      "User 648, total time: 1.13\n",
      "User 649, total time: 1.35\n",
      "User 650, total time: 1.92\n",
      "User 651, total time: 1.13\n",
      "User 652, total time: 1.21\n",
      "User 653, total time: 1.07\n",
      "User 654, total time: 1.08\n",
      "User 655, total time: 1.13\n",
      "User 656, total time: 1.06\n",
      "User 657, total time: 1.17\n",
      "User 658, total time: 1.22\n",
      "User 659, total time: 1.50\n",
      "User 660, total time: 1.45\n",
      "User 661, total time: 1.09\n",
      "User 662, total time: 1.14\n",
      "User 663, total time: 1.36\n",
      "User 664, total time: 1.16\n",
      "User 665, total time: 1.85\n",
      "User 666, total time: 1.30\n",
      "User 667, total time: 1.61\n",
      "User 668, total time: 1.13\n",
      "User 669, total time: 1.00\n",
      "User 670, total time: 1.28\n",
      "User 671, total time: 1.08\n",
      "User 672, total time: 1.19\n",
      "User 673, total time: 1.21\n",
      "User 674, total time: 1.07\n",
      "User 675, total time: 1.07\n",
      "User 676, total time: 1.15\n",
      "User 677, total time: 1.13\n",
      "User 678, total time: 1.23\n",
      "User 679, total time: 1.08\n",
      "User 680, total time: 1.06\n",
      "User 681, total time: 1.06\n",
      "User 682, total time: 1.08\n",
      "User 683, total time: 1.09\n",
      "User 684, total time: 1.20\n",
      "User 685, total time: 1.03\n",
      "User 686, total time: 1.09\n",
      "User 687, total time: 1.25\n",
      "User 688, total time: 0.99\n",
      "User 689, total time: 1.11\n",
      "User 690, total time: 1.14\n",
      "User 691, total time: 1.13\n",
      "User 692, total time: 1.28\n",
      "User 693, total time: 1.09\n",
      "User 694, total time: 1.19\n",
      "User 695, total time: 1.27\n",
      "User 696, total time: 1.33\n",
      "User 697, total time: 1.36\n",
      "User 698, total time: 1.17\n",
      "User 699, total time: 1.09\n",
      "User 700, total time: 1.16\n",
      "User 701, total time: 1.20\n",
      "User 702, total time: 1.31\n",
      "User 703, total time: 1.34\n",
      "User 704, total time: 1.72\n",
      "User 705, total time: 1.16\n",
      "User 706, total time: 1.29\n",
      "User 707, total time: 1.39\n",
      "User 708, total time: 1.19\n",
      "User 709, total time: 1.30\n",
      "User 710, total time: 1.47\n",
      "User 711, total time: 1.05\n",
      "User 712, total time: 1.16\n",
      "User 713, total time: 1.00\n",
      "User 714, total time: 1.02\n",
      "User 715, total time: 1.27\n",
      "User 716, total time: 1.38\n",
      "User 717, total time: 1.00\n",
      "User 718, total time: 1.16\n",
      "User 719, total time: 1.19\n",
      "User 720, total time: 1.11\n",
      "User 721, total time: 1.06\n",
      "User 722, total time: 1.20\n",
      "User 723, total time: 1.00\n",
      "User 724, total time: 1.11\n",
      "User 725, total time: 1.09\n",
      "User 726, total time: 1.03\n",
      "User 727, total time: 1.06\n",
      "User 728, total time: 1.11\n",
      "User 729, total time: 1.41\n",
      "User 730, total time: 1.30\n",
      "User 731, total time: 1.36\n",
      "User 732, total time: 1.22\n",
      "User 733, total time: 1.02\n",
      "User 734, total time: 1.03\n",
      "User 735, total time: 0.99\n",
      "User 736, total time: 1.02\n",
      "User 737, total time: 1.00\n",
      "User 738, total time: 1.08\n",
      "User 739, total time: 1.17\n",
      "User 740, total time: 1.04\n",
      "User 741, total time: 1.25\n",
      "User 742, total time: 1.19\n",
      "User 743, total time: 1.16\n",
      "User 744, total time: 1.05\n",
      "User 745, total time: 1.20\n",
      "User 746, total time: 1.17\n",
      "User 747, total time: 1.44\n",
      "User 748, total time: 1.11\n",
      "User 749, total time: 1.36\n",
      "User 750, total time: 1.25\n",
      "User 751, total time: 1.28\n",
      "User 752, total time: 1.36\n",
      "User 753, total time: 1.22\n",
      "User 754, total time: 1.19\n",
      "User 755, total time: 1.20\n",
      "User 756, total time: 1.22\n",
      "User 757, total time: 1.20\n",
      "User 758, total time: 1.13\n",
      "User 759, total time: 1.05\n",
      "User 760, total time: 1.13\n",
      "User 761, total time: 1.19\n",
      "User 762, total time: 1.13\n",
      "User 763, total time: 1.16\n",
      "User 764, total time: 1.53\n",
      "User 765, total time: 1.36\n",
      "User 766, total time: 1.41\n",
      "User 767, total time: 1.02\n",
      "User 768, total time: 1.03\n",
      "User 769, total time: 1.09\n",
      "User 770, total time: 1.09\n",
      "User 771, total time: 1.27\n",
      "User 772, total time: 1.11\n",
      "User 773, total time: 1.13\n",
      "User 774, total time: 1.33\n",
      "User 775, total time: 1.23\n",
      "User 776, total time: 1.02\n",
      "User 777, total time: 1.17\n",
      "User 778, total time: 1.28\n",
      "User 779, total time: 1.30\n",
      "User 780, total time: 1.08\n",
      "User 781, total time: 1.16\n",
      "User 782, total time: 1.16\n",
      "User 783, total time: 1.23\n",
      "User 784, total time: 1.13\n",
      "User 785, total time: 1.02\n",
      "User 786, total time: 1.61\n",
      "User 787, total time: 1.22\n",
      "User 788, total time: 1.59\n",
      "User 789, total time: 1.19\n",
      "User 790, total time: 1.08\n",
      "User 791, total time: 1.06\n",
      "User 792, total time: 1.11\n",
      "User 793, total time: 1.41\n",
      "User 794, total time: 1.14\n",
      "User 795, total time: 1.09\n",
      "User 796, total time: 1.45\n",
      "User 797, total time: 1.13\n",
      "User 798, total time: 1.09\n",
      "User 799, total time: 1.28\n",
      "User 800, total time: 1.38\n",
      "User 801, total time: 1.09\n",
      "User 802, total time: 1.19\n",
      "User 803, total time: 1.16\n",
      "User 804, total time: 1.13\n",
      "User 805, total time: 1.39\n",
      "User 806, total time: 1.25\n",
      "User 807, total time: 1.18\n",
      "User 808, total time: 1.91\n",
      "User 809, total time: 1.03\n",
      "User 810, total time: 1.33\n",
      "User 811, total time: 1.09\n",
      "User 812, total time: 1.09\n",
      "User 813, total time: 1.11\n",
      "User 814, total time: 1.38\n",
      "User 815, total time: 1.02\n",
      "User 816, total time: 1.00\n",
      "User 817, total time: 1.02\n",
      "User 818, total time: 1.14\n",
      "User 819, total time: 1.03\n",
      "User 820, total time: 1.03\n",
      "User 821, total time: 1.00\n",
      "User 822, total time: 1.09\n",
      "User 823, total time: 1.05\n",
      "User 824, total time: 1.14\n",
      "User 825, total time: 1.30\n",
      "User 826, total time: 1.09\n",
      "User 827, total time: 1.09\n",
      "User 828, total time: 1.23\n",
      "User 829, total time: 1.53\n",
      "User 830, total time: 1.38\n",
      "User 831, total time: 1.05\n",
      "User 832, total time: 1.02\n",
      "User 833, total time: 0.98\n",
      "User 834, total time: 1.14\n",
      "User 835, total time: 1.00\n",
      "User 836, total time: 1.03\n",
      "User 837, total time: 1.14\n",
      "User 838, total time: 1.08\n",
      "User 839, total time: 1.24\n",
      "User 840, total time: 1.21\n",
      "User 841, total time: 1.75\n",
      "User 842, total time: 1.06\n",
      "User 843, total time: 1.22\n",
      "User 844, total time: 1.03\n",
      "User 845, total time: 1.33\n",
      "User 846, total time: 1.13\n",
      "User 847, total time: 1.20\n",
      "User 848, total time: 1.11\n",
      "User 849, total time: 1.22\n",
      "User 850, total time: 1.06\n",
      "User 851, total time: 1.03\n",
      "User 852, total time: 1.39\n",
      "User 853, total time: 1.56\n",
      "User 854, total time: 1.41\n",
      "User 855, total time: 1.33\n",
      "User 856, total time: 1.30\n",
      "User 857, total time: 1.49\n",
      "User 858, total time: 1.08\n",
      "User 859, total time: 1.23\n",
      "User 860, total time: 1.55\n",
      "User 861, total time: 1.03\n",
      "User 862, total time: 1.23\n",
      "User 863, total time: 1.06\n",
      "User 864, total time: 1.08\n",
      "User 865, total time: 1.31\n",
      "User 866, total time: 1.03\n",
      "User 867, total time: 1.05\n",
      "User 868, total time: 1.19\n",
      "User 869, total time: 1.14\n",
      "User 870, total time: 1.16\n",
      "User 871, total time: 1.00\n",
      "User 872, total time: 1.42\n",
      "User 873, total time: 1.16\n",
      "User 874, total time: 1.05\n",
      "User 875, total time: 1.17\n",
      "User 876, total time: 1.06\n",
      "User 877, total time: 1.38\n",
      "User 878, total time: 1.03\n",
      "User 879, total time: 1.00\n",
      "User 880, total time: 1.13\n",
      "User 881, total time: 1.17\n",
      "User 882, total time: 1.25\n",
      "User 883, total time: 1.44\n",
      "User 884, total time: 1.11\n",
      "User 885, total time: 1.38\n",
      "User 886, total time: 1.44\n",
      "User 887, total time: 1.22\n",
      "User 888, total time: 1.19\n",
      "User 889, total time: 1.23\n",
      "User 890, total time: 1.08\n",
      "User 891, total time: 1.02\n",
      "User 892, total time: 1.08\n",
      "User 893, total time: 1.11\n",
      "User 894, total time: 1.36\n",
      "User 895, total time: 1.41\n",
      "User 896, total time: 1.08\n",
      "User 897, total time: 1.09\n",
      "User 898, total time: 1.05\n",
      "User 899, total time: 1.56\n",
      "User 900, total time: 1.45\n",
      "User 901, total time: 1.14\n",
      "User 902, total time: 1.27\n",
      "User 903, total time: 1.03\n",
      "User 904, total time: 1.16\n",
      "User 905, total time: 1.13\n",
      "User 906, total time: 1.08\n",
      "User 907, total time: 1.27\n",
      "User 908, total time: 1.45\n",
      "User 909, total time: 1.19\n",
      "User 910, total time: 1.14\n",
      "User 911, total time: 1.06\n",
      "User 912, total time: 1.22\n",
      "User 913, total time: 1.41\n",
      "User 914, total time: 1.20\n",
      "User 915, total time: 1.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 916, total time: 1.19\n",
      "User 917, total time: 1.09\n",
      "User 918, total time: 1.55\n",
      "User 919, total time: 1.16\n",
      "User 920, total time: 1.11\n",
      "User 921, total time: 1.14\n",
      "User 922, total time: 1.12\n",
      "User 923, total time: 1.30\n",
      "User 924, total time: 1.01\n",
      "User 925, total time: 1.08\n",
      "User 926, total time: 1.09\n",
      "User 927, total time: 1.32\n",
      "User 928, total time: 1.05\n",
      "User 929, total time: 1.05\n",
      "User 930, total time: 1.13\n",
      "User 931, total time: 1.07\n",
      "User 932, total time: 1.01\n",
      "User 933, total time: 1.03\n",
      "User 934, total time: 1.17\n",
      "User 935, total time: 1.00\n",
      "User 936, total time: 1.38\n",
      "User 937, total time: 1.00\n",
      "User 938, total time: 1.52\n",
      "User 939, total time: 1.13\n",
      "User 940, total time: 1.35\n",
      "User 941, total time: 1.09\n",
      "User 942, total time: 1.09\n",
      "User 943, total time: 1.25\n",
      "User 944, total time: 1.23\n",
      "User 945, total time: 1.16\n",
      "User 946, total time: 1.24\n",
      "User 947, total time: 1.31\n",
      "User 948, total time: 1.39\n",
      "User 949, total time: 1.38\n",
      "User 950, total time: 1.28\n",
      "User 951, total time: 1.11\n",
      "User 952, total time: 1.09\n",
      "User 953, total time: 1.27\n",
      "User 954, total time: 1.06\n",
      "User 955, total time: 1.28\n",
      "User 956, total time: 1.16\n",
      "User 957, total time: 1.33\n",
      "User 958, total time: 1.24\n",
      "User 959, total time: 1.11\n",
      "User 960, total time: 1.17\n",
      "User 961, total time: 1.22\n",
      "User 962, total time: 1.17\n",
      "User 963, total time: 1.08\n",
      "User 964, total time: 1.28\n",
      "User 965, total time: 1.28\n",
      "User 966, total time: 1.02\n",
      "User 967, total time: 1.63\n",
      "User 968, total time: 1.30\n",
      "User 969, total time: 1.41\n",
      "User 970, total time: 1.06\n",
      "User 971, total time: 1.13\n",
      "User 972, total time: 1.03\n",
      "User 973, total time: 1.06\n",
      "User 974, total time: 1.22\n",
      "User 975, total time: 1.14\n",
      "User 976, total time: 1.03\n",
      "User 977, total time: 1.02\n",
      "User 978, total time: 1.06\n",
      "User 979, total time: 1.84\n",
      "User 980, total time: 1.11\n",
      "User 981, total time: 1.05\n",
      "User 982, total time: 1.11\n",
      "User 983, total time: 0.95\n",
      "User 984, total time: 0.97\n",
      "User 985, total time: 1.14\n",
      "User 986, total time: 1.19\n",
      "User 987, total time: 1.14\n",
      "User 988, total time: 1.09\n",
      "User 989, total time: 0.98\n",
      "User 990, total time: 0.95\n",
      "User 991, total time: 1.19\n",
      "User 992, total time: 1.31\n",
      "User 993, total time: 1.09\n",
      "User 994, total time: 2.00\n",
      "User 995, total time: 1.34\n",
      "User 996, total time: 1.31\n",
      "User 997, total time: 1.16\n",
      "User 998, total time: 1.22\n",
      "User 999, total time: 1.06\n",
      "1000\n",
      "User 1000, total time: 1.05\n",
      "User 1001, total time: 1.30\n",
      "User 1002, total time: 1.33\n",
      "User 1003, total time: 1.47\n",
      "User 1004, total time: 1.39\n",
      "User 1005, total time: 1.55\n",
      "User 1006, total time: 1.05\n",
      "User 1007, total time: 1.16\n",
      "User 1008, total time: 1.19\n",
      "User 1009, total time: 1.14\n",
      "User 1010, total time: 1.20\n",
      "User 1011, total time: 1.08\n",
      "User 1012, total time: 1.19\n",
      "User 1013, total time: 2.05\n",
      "User 1014, total time: 1.66\n",
      "User 1015, total time: 1.09\n",
      "User 1016, total time: 0.98\n",
      "User 1017, total time: 1.02\n",
      "User 1018, total time: 1.86\n",
      "User 1019, total time: 1.09\n",
      "User 1020, total time: 1.06\n",
      "User 1021, total time: 0.99\n",
      "User 1022, total time: 1.03\n",
      "User 1023, total time: 0.98\n",
      "User 1024, total time: 1.83\n",
      "User 1025, total time: 1.14\n",
      "User 1026, total time: 1.02\n",
      "User 1027, total time: 1.22\n",
      "User 1028, total time: 1.03\n",
      "User 1029, total time: 1.00\n",
      "User 1030, total time: 1.08\n",
      "User 1031, total time: 1.20\n",
      "User 1032, total time: 1.00\n",
      "User 1033, total time: 1.42\n",
      "User 1034, total time: 1.02\n",
      "User 1035, total time: 1.02\n",
      "User 1036, total time: 1.06\n",
      "User 1037, total time: 0.98\n",
      "User 1038, total time: 1.00\n",
      "User 1039, total time: 1.23\n",
      "User 1040, total time: 1.28\n",
      "User 1041, total time: 1.81\n",
      "User 1042, total time: 1.05\n",
      "User 1043, total time: 1.28\n",
      "User 1044, total time: 1.13\n",
      "User 1045, total time: 1.31\n",
      "User 1046, total time: 2.28\n",
      "User 1047, total time: 1.06\n",
      "User 1048, total time: 1.42\n",
      "User 1049, total time: 1.16\n",
      "User 1050, total time: 1.14\n",
      "User 1051, total time: 1.08\n",
      "User 1052, total time: 1.16\n",
      "User 1053, total time: 1.77\n",
      "User 1054, total time: 1.11\n",
      "User 1055, total time: 1.08\n",
      "User 1056, total time: 1.08\n",
      "User 1057, total time: 1.09\n",
      "User 1058, total time: 1.16\n",
      "User 1059, total time: 1.14\n",
      "User 1060, total time: 1.41\n",
      "User 1061, total time: 1.20\n",
      "User 1062, total time: 1.17\n",
      "User 1063, total time: 1.53\n",
      "User 1064, total time: 1.08\n",
      "User 1065, total time: 1.39\n",
      "User 1066, total time: 1.30\n",
      "User 1067, total time: 1.11\n",
      "User 1068, total time: 1.13\n",
      "User 1069, total time: 1.16\n",
      "User 1070, total time: 1.11\n",
      "User 1071, total time: 1.04\n",
      "User 1072, total time: 1.20\n",
      "User 1073, total time: 1.02\n",
      "User 1074, total time: 1.06\n",
      "User 1075, total time: 1.25\n",
      "User 1076, total time: 1.13\n",
      "User 1077, total time: 1.21\n",
      "User 1078, total time: 1.15\n",
      "User 1079, total time: 1.30\n",
      "User 1080, total time: 1.20\n",
      "User 1081, total time: 1.47\n",
      "User 1082, total time: 1.03\n",
      "User 1083, total time: 1.45\n",
      "User 1084, total time: 1.05\n",
      "User 1085, total time: 1.45\n",
      "User 1086, total time: 1.02\n",
      "User 1087, total time: 1.20\n",
      "User 1088, total time: 1.22\n",
      "User 1089, total time: 1.14\n",
      "User 1090, total time: 1.14\n",
      "User 1091, total time: 1.10\n",
      "User 1092, total time: 1.13\n",
      "User 1093, total time: 1.54\n",
      "User 1094, total time: 1.11\n",
      "User 1095, total time: 1.05\n",
      "User 1096, total time: 1.22\n",
      "User 1097, total time: 1.22\n",
      "User 1098, total time: 1.11\n",
      "User 1099, total time: 1.14\n",
      "User 1100, total time: 1.29\n",
      "User 1101, total time: 1.07\n",
      "User 1102, total time: 1.30\n",
      "User 1103, total time: 1.23\n",
      "User 1104, total time: 1.27\n",
      "User 1105, total time: 1.32\n",
      "User 1106, total time: 1.48\n",
      "User 1107, total time: 1.29\n",
      "User 1108, total time: 1.19\n",
      "User 1109, total time: 1.13\n",
      "User 1110, total time: 1.29\n",
      "User 1111, total time: 1.07\n",
      "User 1112, total time: 1.17\n",
      "User 1113, total time: 1.45\n",
      "User 1114, total time: 1.13\n",
      "User 1115, total time: 1.26\n",
      "User 1116, total time: 1.11\n",
      "User 1117, total time: 1.21\n",
      "User 1118, total time: 1.04\n",
      "User 1119, total time: 1.04\n",
      "User 1120, total time: 1.04\n",
      "User 1121, total time: 1.07\n",
      "User 1122, total time: 1.07\n",
      "User 1123, total time: 1.06\n",
      "User 1124, total time: 1.14\n",
      "User 1125, total time: 0.99\n",
      "User 1126, total time: 1.08\n",
      "User 1127, total time: 1.03\n",
      "User 1128, total time: 1.05\n",
      "User 1129, total time: 1.00\n",
      "User 1130, total time: 1.22\n",
      "User 1131, total time: 1.66\n",
      "User 1132, total time: 1.35\n",
      "User 1133, total time: 1.07\n",
      "User 1134, total time: 1.10\n",
      "User 1135, total time: 1.52\n",
      "User 1136, total time: 1.10\n",
      "User 1137, total time: 1.03\n",
      "User 1138, total time: 1.03\n",
      "User 1139, total time: 1.35\n",
      "User 1140, total time: 1.51\n",
      "User 1141, total time: 1.27\n",
      "User 1142, total time: 1.16\n",
      "User 1143, total time: 1.33\n",
      "User 1144, total time: 1.12\n",
      "User 1145, total time: 1.13\n",
      "User 1146, total time: 1.07\n",
      "User 1147, total time: 1.07\n",
      "User 1148, total time: 1.24\n",
      "User 1149, total time: 1.40\n",
      "User 1150, total time: 1.13\n",
      "User 1151, total time: 1.19\n",
      "User 1152, total time: 1.40\n",
      "User 1153, total time: 1.58\n",
      "User 1154, total time: 1.03\n",
      "User 1155, total time: 1.17\n",
      "User 1156, total time: 1.16\n",
      "User 1157, total time: 1.16\n",
      "User 1158, total time: 1.11\n",
      "User 1159, total time: 1.14\n",
      "User 1160, total time: 1.21\n",
      "User 1161, total time: 1.19\n",
      "User 1162, total time: 1.52\n",
      "User 1163, total time: 1.19\n",
      "User 1164, total time: 1.12\n",
      "User 1165, total time: 1.10\n",
      "User 1166, total time: 1.13\n",
      "User 1167, total time: 1.02\n",
      "User 1168, total time: 1.05\n",
      "User 1169, total time: 1.40\n",
      "User 1170, total time: 1.02\n",
      "User 1171, total time: 1.10\n",
      "User 1172, total time: 1.13\n",
      "User 1173, total time: 1.00\n",
      "User 1174, total time: 1.08\n",
      "User 1175, total time: 1.07\n",
      "User 1176, total time: 1.05\n",
      "User 1177, total time: 1.18\n",
      "User 1178, total time: 1.04\n",
      "User 1179, total time: 1.12\n",
      "User 1180, total time: 1.13\n",
      "User 1181, total time: 1.02\n",
      "User 1182, total time: 1.47\n",
      "User 1183, total time: 0.99\n",
      "User 1184, total time: 0.99\n",
      "User 1185, total time: 1.00\n",
      "User 1186, total time: 1.11\n",
      "User 1187, total time: 1.07\n",
      "User 1188, total time: 1.18\n",
      "User 1189, total time: 1.03\n",
      "User 1190, total time: 1.00\n",
      "User 1191, total time: 1.00\n",
      "User 1192, total time: 1.03\n",
      "User 1193, total time: 1.08\n",
      "User 1194, total time: 1.26\n",
      "User 1195, total time: 1.33\n",
      "User 1196, total time: 1.37\n",
      "User 1197, total time: 1.11\n",
      "User 1198, total time: 1.05\n",
      "User 1199, total time: 1.13\n",
      "User 1200, total time: 1.08\n",
      "User 1201, total time: 1.21\n",
      "User 1202, total time: 1.35\n",
      "User 1203, total time: 1.36\n",
      "User 1204, total time: 1.16\n",
      "User 1205, total time: 1.13\n",
      "User 1206, total time: 1.13\n",
      "User 1207, total time: 1.11\n"
     ]
    }
   ],
   "source": [
    "create_dictionaries = True\n",
    "\n",
    "if create_dictionaries:\n",
    "    import time\n",
    "    recommender.eval()\n",
    "    # Evaluate the model on the test set\n",
    "    \n",
    "    jaccard_expl_dict = {}\n",
    "    cosine_expl_dict = {}\n",
    "    pop_expl_dict = {}\n",
    "    lime_expl_dict = {}\n",
    "    tf_idf_expl_dict = {}\n",
    "    lire_expl_dict = {}\n",
    "    fia_expl_dict = {}\n",
    "    accent_expl_dict = {}\n",
    "    lxr_expl_dict = {}\n",
    "    \n",
    "#     shap_expl_dict = {}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_array.shape[0]):\n",
    "            if i%500 == 0:\n",
    "                print(i)\n",
    "            start_time = time.time()\n",
    "            user_vector = test_array[i]\n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            user_id = int(test_data.index[i])\n",
    "\n",
    "            item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "            item_vector =  items_array[item_id]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "            user_vector[item_id] = 0\n",
    "            user_tensor[item_id] = 0\n",
    "\n",
    "            jaccard_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'jaccard')\n",
    "            cosine_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'cosine')\n",
    "            pop_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'pop')\n",
    "            lime_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'lime')\n",
    "            tf_idf_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'tf_idf', user_id=user_id)\n",
    "            lire_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'lire')\n",
    "            fia_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'fia')\n",
    "            accent_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'accent')\n",
    "            lxr_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'lxr')\n",
    "#             shap_expl_dict[user_id] = single_user_expl(user_vector, item_id, num_items, recommender, mask_type= 'shap',user_id = user_id)\n",
    "            prev_time = time.time()\n",
    "            print(\"User {}, total time: {:.2f}\".format(i,prev_time - start_time))\n",
    "\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_jaccard_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(jaccard_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_cosine_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(cosine_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_pop_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(pop_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lime_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(lime_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_tf_idf_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(tf_idf_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lire_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(lire_expl_dict, handle)\n",
    "    \n",
    "        with open(Path(files_path,f'{recommender_name}_fia_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(fia_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_accent_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(accent_expl_dict, handle) \n",
    "            \n",
    "        with open(Path(files_path,f'{recommender_name}_lxr_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(lxr_expl_dict, handle)\n",
    "\n",
    "    #     with open(Path(files_path,f'{recommender_name}_shap_expl_dict.pkl'), 'wb') as handle:\n",
    "    #         pickle.dump(shap_expl_dict, handle)\n",
    "\n",
    "\n",
    "else:\n",
    "        with open(Path(files_path,f'{recommender_name}_jaccard_expl_dict.pkl'), 'rb') as handle:\n",
    "            jaccard_expl_dict = pickle.load(handle) \n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_cosine_expl_dict.pkl'), 'rb') as handle:\n",
    "            cosine_expl_dict = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_pop_expl_dict.pkl'), 'rb') as handle:\n",
    "            pop_expl_dict = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lime_expl_dict.pkl'), 'rb') as handle:\n",
    "            lime_expl_dict = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_tf_idf_expl_dict.pkl'), 'rb') as handle:\n",
    "            tf_idf_expl_dict = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lire_expl_dict_200_clip.pkl'), 'rb') as handle:\n",
    "            lire_expl_dict_200_clip = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_fia_expl_dict.pkl'), 'rb') as handle:\n",
    "            fia_expl_dict = pickle.load(handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lxr_expl_dict.pkl'), 'rb') as handle:\n",
    "            lxr_expl_dict = pickle.load(handle)\n",
    "\n",
    "    #     with open(Path(files_path,f'{recommender_name}_shap_expl_dict.pkl'), 'rb') as handle:\n",
    "    #         shap_expl_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_expl_type(expl_name):\n",
    "    print(f' ============ Start explaining by {expl_name} ============')\n",
    "    with open(Path(files_path,f'{recommender_name}_{expl_name}_expl_dict.pkl'), 'rb') as handle:\n",
    "        expl_dict = pickle.load(handle)\n",
    "    recommender.eval()\n",
    "    # Evaluate the model on the test set\n",
    "    num_of_bins = 11\n",
    "\n",
    "\n",
    "    users_DEL = np.zeros(num_of_bins)\n",
    "    users_INS = np.zeros(num_of_bins)\n",
    "    reciprocal = np.zeros(num_of_bins)\n",
    "    NDCG = np.zeros(num_of_bins)\n",
    "    POS_at_1 = np.zeros(num_of_bins)\n",
    "    POS_at_5 = np.zeros(num_of_bins)\n",
    "    POS_at_10 = np.zeros(num_of_bins)\n",
    "    POS_at_20 = np.zeros(num_of_bins)\n",
    "    POS_at_50 = np.zeros(num_of_bins)\n",
    "    POS_at_100 = np.zeros(num_of_bins)\n",
    "    NEG_at_1 = np.zeros(num_of_bins)\n",
    "    NEG_at_5 = np.zeros(num_of_bins)\n",
    "    NEG_at_10 = np.zeros(num_of_bins)\n",
    "    NEG_at_20 = np.zeros(num_of_bins)\n",
    "    NEG_at_50 = np.zeros(num_of_bins)\n",
    "    NEG_at_100 = np.zeros(num_of_bins)\n",
    "    rank_at_1 = np.zeros(num_of_bins)\n",
    "    rank_at_5 = np.zeros(num_of_bins)\n",
    "    rank_at_10 = np.zeros(num_of_bins)\n",
    "    rank_at_20 = np.zeros(num_of_bins)\n",
    "    rank_at_50 = np.zeros(num_of_bins)\n",
    "    rank_at_100 = np.zeros(num_of_bins)\n",
    "\n",
    "    num_of_bins=10\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_array.shape[0]):\n",
    "            start_time = time.time()\n",
    "            user_vector = test_array[i]\n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            user_id = int(test_data.index[i])\n",
    "\n",
    "            item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "            item_vector =  items_array[item_id]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "            user_vector[item_id] = 0\n",
    "            user_tensor[item_id] = 0\n",
    "\n",
    "            user_expl = expl_dict[user_id]\n",
    "\n",
    "            res = single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender, user_expl, **kw_dict)\n",
    "            users_DEL += res[0]\n",
    "            users_INS += res[1]\n",
    "            reciprocal += res[2]\n",
    "            NDCG += res[3]\n",
    "            POS_at_1 += res[4]\n",
    "            POS_at_5 += res[5]\n",
    "            POS_at_10 += res[6]\n",
    "            POS_at_20 += res[7]\n",
    "            POS_at_50 += res[8]\n",
    "            POS_at_100 += res[9]\n",
    "            NEG_at_1 += res[10]\n",
    "            NEG_at_5 += res[11]\n",
    "            NEG_at_10 += res[12]\n",
    "            NEG_at_20 += res[13]\n",
    "            NEG_at_50 += res[14]\n",
    "            NEG_at_100 += res[15]\n",
    "            rank_at_1 += res[16]\n",
    "            rank_at_5 += res[17]\n",
    "            rank_at_10 += res[18]\n",
    "            rank_at_20 += res[19]\n",
    "            rank_at_50 += res[20]\n",
    "            rank_at_100 += res[21]\n",
    "\n",
    "            if i%500 == 0:\n",
    "                prev_time = time.time()\n",
    "                print(\"User {}, total time: {:.2f}\".format(i,prev_time - start_time))\n",
    "\n",
    "    a = i+1\n",
    "\n",
    "    print(f'users_DEL_{expl_name}: ', np.mean(users_DEL[1:])/a)\n",
    "    print(f'users_INS_{expl_name}: ', np.mean(users_INS[1:])/a)\n",
    "    print(f'reciprocal_{expl_name}: ', np.mean(reciprocal[1:])/a)\n",
    "    print(f'NDCG_{expl_name}: ', np.mean(NDCG[1:])/a)\n",
    "    print(f'POS_at_1_{expl_name}: ', np.mean(POS_at_1[1:])/a)\n",
    "    print(f'POS_at_5_{expl_name}: ', np.mean(POS_at_5[1:])/a)\n",
    "    print(f'POS_at_10_{expl_name}: ', np.mean(POS_at_10[1:])/a)\n",
    "    print(f'POS_at_20_{expl_name}: ', np.mean(POS_at_20[1:])/a)\n",
    "    print(f'POS_at_50_{expl_name}: ', np.mean(POS_at_50[1:])/a)\n",
    "    print(f'POS_at_100_{expl_name}: ', np.mean(POS_at_100[1:])/a)\n",
    "    print(f'NEG_at_1_{expl_name}: ', np.mean(NEG_at_1[1:])/a)\n",
    "    print(f'NEG_at_5_{expl_name}: ', np.mean(NEG_at_5[1:])/a)\n",
    "    print(f'NEG_at_10_{expl_name}: ', np.mean(NEG_at_10[1:])/a)\n",
    "    print(f'NEG_at_20_{expl_name}: ', np.mean(NEG_at_20[1:])/a)\n",
    "    print(f'NEG_at_50_{expl_name}: ', np.mean(NEG_at_50[1:])/a)\n",
    "    print(f'NEG_at_100_{expl_name}: ', np.mean(NEG_at_100[1:])/a)\n",
    "    print(f'rank_at_1_{expl_name}: ', np.mean(rank_at_1[1:])/a)\n",
    "    print(f'rank_at_5_{expl_name}: ', np.mean(rank_at_5[1:])/a)\n",
    "    print(f'rank_at_10_{expl_name}: ', np.mean(rank_at_10[1:])/a)\n",
    "    print(f'rank_at_20_{expl_name}: ', np.mean(rank_at_20[1:])/a)\n",
    "    print(f'rank_at_50_{expl_name}: ', np.mean(rank_at_50[1:])/a)\n",
    "    print(f'rank_at_100_{expl_name}: ', np.mean(rank_at_100[1:])/a)\n",
    "    \n",
    "    print(np.mean(users_DEL[1:])/a , np.mean(users_INS[1:])/a , np.mean(reciprocal[1:])/a , np.mean(NDCG[1:])/a , np.mean(POS_at_1[1:])/a , np.mean(NEG_at_1[1:])/a , np.mean(rank_at_1[1:])/a , np.mean(POS_at_5[1:])/a , np.mean(NEG_at_5[1:])/a , np.mean(rank_at_5[1:])/a , np.mean(POS_at_10[1:])/a , np.mean(NEG_at_10[1:])/a , np.mean(rank_at_10[1:])/a , np.mean(POS_at_20[1:])/a , np.mean(NEG_at_20[1:])/a , np.mean(rank_at_20[1:])/a , np.mean(POS_at_50[1:])/a , np.mean(NEG_at_50[1:])/a , np.mean(rank_at_50[1:])/a , np.mean(POS_at_100[1:])/a , np.mean(NEG_at_100[1:])/a , np.mean(rank_at_100[1:])/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10d66300",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_names_list = ['tf_idf','pop','lime','lire','fia','accent','lxr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "056c4d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for expl_name in expl_names_list:\n",
    "    eval_one_expl_type(expl_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
