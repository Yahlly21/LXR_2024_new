{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df26b726-af5c-40da-b1f7-0a2798e2ccbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports and initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af063889-a1b7-4f7c-bf9d-6fe7d6865f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:44.224449Z",
     "iopub.status.busy": "2024-04-14T18:27:44.224025Z",
     "iopub.status.idle": "2024-04-14T18:27:44.232844Z",
     "shell.execute_reply": "2024-04-14T18:27:44.232177Z",
     "shell.execute_reply.started": "2024-04-14T18:27:44.224413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import importlib\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33132c9c-2808-44b5-b272-2c523999fe2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:44.375998Z",
     "iopub.status.busy": "2024-04-14T18:27:44.375641Z",
     "iopub.status.idle": "2024-04-14T18:27:44.381111Z",
     "shell.execute_reply": "2024-04-14T18:27:44.380198Z",
     "shell.execute_reply.started": "2024-04-14T18:27:44.375964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_dir = Path(os.getcwd())\n",
    "checkpoints_path = Path(export_dir, \"checkpoints_new\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1f2b6a-0b86-4968-85e1-530dda768b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:44.561518Z",
     "iopub.status.busy": "2024-04-14T18:27:44.561059Z",
     "iopub.status.idle": "2024-04-14T18:27:44.567376Z",
     "shell.execute_reply": "2024-04-14T18:27:44.566638Z",
     "shell.execute_reply.started": "2024-04-14T18:27:44.561481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7eb549-f84b-4029-b534-2748002cc882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:44.730887Z",
     "iopub.status.busy": "2024-04-14T18:27:44.730458Z",
     "iopub.status.idle": "2024-04-14T18:27:44.746318Z",
     "shell.execute_reply": "2024-04-14T18:27:44.745599Z",
     "shell.execute_reply.started": "2024-04-14T18:27:44.730851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\",\n",
    "    \"NCF\": \"single\",\n",
    "    \"MLP_model\": \"single\",\n",
    "    \"GMF_model\": \"single\"\n",
    "}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"ML1M_demographic\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155\n",
    "}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"ML1M_demographic\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362\n",
    "}\n",
    "\n",
    "demographic_dict = {\n",
    "    \"ML1M_demographic\": True,\n",
    "    \"ML1M\":False,\n",
    "    \"Yahoo\":False, \n",
    "    \"Pinterest\":False\n",
    "}\n",
    "\n",
    "features_dict = {\n",
    "    \"ML1M_demographic\": 3421,\n",
    "    \"ML1M\":None,\n",
    "    \"Yahoo\":None, \n",
    "    \"Pinterest\":None\n",
    "}\n",
    "\n",
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),\n",
    "    (\"ML1M\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_ML1M_0.0001_64_27.pt\"),\n",
    "    (\"ML1M\",\"GMF_model\"): Path(checkpoints_path, \"GMF_best_ML1M_0.0001_32_17.pt\"),\n",
    "    (\"ML1M\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_5e-05_64_16.pt\"),\n",
    "\n",
    "    (\"ML1M_demographic\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_demographic_0.0001_64_6_18.pt\"),\n",
    "    (\"ML1M_demographic\",\"MLP\"):Path(checkpoints_path, \"MLP_ML1M_demographic_0.0_64_0_28.pt\"),\n",
    "    (\"ML1M_demographic\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model2_ML1M_demographic_7e-05_128_3_22.pt\"),\n",
    "    (\"ML1M_demographic\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model_ML1M_demographic_0.00061_64_21_10.pt\"),\n",
    "    (\"ML1M_demographic\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_demographic_0.00023_32_3_2.pt\"),\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    (\"Yahoo\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_Yahoo_5e-05_32_29.pt\"),\n",
    "    (\"Yahoo\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model2_Yahoo_0.0_128_0_49.pt\"),\n",
    "    (\"Yahoo\",\"NCF\"):Path(checkpoints_path, \"NCF_Yahoo_0.001_64_21_0.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_12_18_0.0001_256.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    (\"Pinterest\",\"MLP_model\"):Path(checkpoints_path, \"MLP_model_Pinterest_4e-05_1024_7_18.pt\"),\n",
    "    (\"Pinterest\",\"GMF_model\"): Path(checkpoints_path, \"GMF_model_Pinterest_0.001_512_3_19.pt\"),\n",
    "    (\"Pinterest\",\"NCF\"):Path(checkpoints_path, \"NCF2_Pinterest_9e-05_32_9_10.pt\"),\n",
    "    \n",
    "}\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "    (\"ML1M\",\"MLP_model\"): 8,\n",
    "    (\"ML1M\",\"GMF_model\"): 8,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "\n",
    "    (\"ML1M_demographic\",\"VAE\"): None,\n",
    "    (\"ML1M_demographic\",\"MLP\"): 32,\n",
    "    (\"ML1M_demographic\",\"MLP_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"GMF_model\"): 8,\n",
    "    (\"ML1M_demographic\",\"NCF\"): 8,\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"MLP_model\"): 8,\n",
    "    (\"Yahoo\",\"GMF_model\"): 8,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"MLP_model\"): 64,\n",
    "    (\"Pinterest\",\"GMF_model\"): 64,\n",
    "    (\"Pinterest\",\"NCF\"): 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01ac2d-653d-486e-9a20-5804b1e79b3c",
   "metadata": {},
   "source": [
    "# Important to edit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6069496e-fdeb-4c91-adc6-50be1fe34931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:45.098084Z",
     "iopub.status.busy": "2024-04-14T18:27:45.097729Z",
     "iopub.status.idle": "2024-04-14T18:27:45.103371Z",
     "shell.execute_reply": "2024-04-14T18:27:45.102212Z",
     "shell.execute_reply.started": "2024-04-14T18:27:45.098050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_names = [\"ML1M\"]\n",
    "#data_names = [\"ML1M\", \"ML1M_demographic\", \"Yahoo\", \"Pinterest\"]\n",
    "\n",
    "recommender_names = [\"MLP\"] # , \"VAE\", \"NCF\"]\n",
    "# recommender_names = [\"MLP\", \"VAE\", \"MLP_model\", \"GMF_model\", \"NCF\"]\n",
    "\n",
    "expl_names_list = ['ip']\n",
    "# expl_names_list = ['jaccard', 'cosine', 'pop', 'tf_idf', 'fia', 'accent', 'lime', 'lire', 'lxr', 'shap', 'ip']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209d1e8-d621-443d-acd7-942a1f80d006",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "939351e5-baeb-4204-83b4-c1acb0f5e851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T13:08:53.251942Z",
     "iopub.status.busy": "2024-04-17T13:08:53.251530Z",
     "iopub.status.idle": "2024-04-17T13:08:53.263044Z",
     "shell.execute_reply": "2024-04-17T13:08:53.262319Z",
     "shell.execute_reply.started": "2024-04-17T13:08:53.251905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "\n",
    "num_of_random_users = 50\n",
    "list_of_nums = [1,2,4,5,8,10,15,20,25,30,35,40,45,50]\n",
    "method = \"sample_random_user_including_std\" #\"sample_random_user_new\", \"sample_random_items\" ,\"sample_random_user\" , \"sample_items_by_pop\", \"sample_only_1_items_by_pop\", \"sample_random_items\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e21c932-1c2c-46ca-98c5-95a0c83110c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:46.782090Z",
     "iopub.status.busy": "2024-04-14T18:27:46.781233Z",
     "iopub.status.idle": "2024-04-14T18:27:46.788150Z",
     "shell.execute_reply": "2024-04-14T18:27:46.787290Z",
     "shell.execute_reply.started": "2024-04-14T18:27:46.782047Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML1M_MLP_sample_random_user_new'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_file_name = f\"{data_names[0]}_{recommender_names[0]}_{method}\"\n",
    "new_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efc769-46e0-470b-9f39-41202b02b7c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions: Do not touch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e751d5-534a-44dd-95ab-23b3ea664447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:49.057460Z",
     "iopub.status.busy": "2024-04-14T18:27:49.057018Z",
     "iopub.status.idle": "2024-04-14T18:27:49.191449Z",
     "shell.execute_reply": "2024-04-14T18:27:49.190647Z",
     "shell.execute_reply.started": "2024-04-14T18:27:49.057406Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "\n",
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *\n",
    "\n",
    "\n",
    "VAE_config= {\n",
    "\"enc_dims\": [512,128],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}\n",
    "\n",
    "\n",
    "Pinterest_VAE_config= {\n",
    "\"enc_dims\": [256,64],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fabcade3-5840-4153-87b7-b56c2caa19a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:49.409476Z",
     "iopub.status.busy": "2024-04-14T18:27:49.409099Z",
     "iopub.status.idle": "2024-04-14T18:27:49.419673Z",
     "shell.execute_reply": "2024-04-14T18:27:49.418873Z",
     "shell.execute_reply.started": "2024-04-14T18:27:49.409435Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name=='MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='VAE':\n",
    "        if data_name == \"Pinterest\":\n",
    "            recommender = VAE(Pinterest_VAE_config, **kw_dict)\n",
    "        else:\n",
    "            recommender = VAE(VAE_config, **kw_dict)\n",
    "    elif recommender_name=='MLP_model':\n",
    "        recommender = MLP_model(hidden_size=hidden_dim, num_layers=3, **kw_dict)\n",
    "    elif recommender_name=='GMF_model':\n",
    "        recommender = GMF_model(hidden_size=hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='NCF':\n",
    "        MLP_temp = MLP_model(hidden_size=hidden_dim, num_layers=3, **kw_dict)\n",
    "        GMF_temp = GMF_model(hidden_size=hidden_dim, **kw_dict)\n",
    "        recommender = NCF(factor_num=hidden_dim, num_layers=3, dropout=0.5, model= 'NeuMF-pre', GMF_model= GMF_temp, MLP_model=MLP_temp, **kw_dict)\n",
    "    \n",
    "    recommender_checkpoint = torch.load(Path(checkpoints_path, recommender_path))\n",
    "    recommender.load_state_dict(recommender_checkpoint)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad= False\n",
    "        \n",
    "    return recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b227b-9eb2-4f4a-ac8f-25111909ce70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New IP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb637c1e-7ab5-43f9-be0e-f4c6ed826bb3",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41a515f-a7fc-48d0-b252-bf9d65188e13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:50.091997Z",
     "iopub.status.busy": "2024-04-14T18:27:50.091627Z",
     "iopub.status.idle": "2024-04-14T18:27:50.099805Z",
     "shell.execute_reply": "2024-04-14T18:27:50.099023Z",
     "shell.execute_reply.started": "2024-04-14T18:27:50.091962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_interpolated_values(baseline, target, num_steps=10):\n",
    "    \"\"\"this function returns a tensor of all the vecrots interpolation steps\"\"\"\n",
    "    baseline = baseline.cpu()\n",
    "    target = user_tensor.cpu()\n",
    "\n",
    "    delta = target - baseline\n",
    "\n",
    "    # Make steps between 0 and 1 \n",
    "    scales = np.linspace(0, 1, num_steps + 1, dtype=np.float32)[:, np.newaxis]\n",
    "        \n",
    "    shape = (num_steps + 1,) + delta.shape\n",
    "    deltas = scales * np.broadcast_to(delta.detach().cpu().numpy(), shape)\n",
    "    interpolated_activations = baseline + deltas\n",
    "\n",
    "    return interpolated_activations  #.to(device).clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c6558b-ea12-44f9-93c4-be8b94149870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:50.454335Z",
     "iopub.status.busy": "2024-04-14T18:27:50.453972Z",
     "iopub.status.idle": "2024-04-14T18:27:50.462935Z",
     "shell.execute_reply": "2024-04-14T18:27:50.462117Z",
     "shell.execute_reply.started": "2024-04-14T18:27:50.454300Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_grads_wrt_to_user_tensor(model, user_tensor, all_items_tensor, item_id):\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Different implementation of the recommenders \n",
    "    if recommender_name == \"VAE\":\n",
    "        preds = model(user_tensor)[0]\n",
    "    else: # MLP or NCF\n",
    "        preds = model(user_tensor, all_items_tensor)\n",
    "    \n",
    "    one_hot = torch.zeros(preds.shape).to(device)\n",
    "    one_hot[item_id] = 1\n",
    "\n",
    "    score = torch.sum(one_hot * preds)\n",
    "    score.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vector_grad = user_tensor.grad.detach()\n",
    "    user_tensor.requires_grad = False\n",
    "    return vector_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3121d-4e46-4d5b-a2a2-c9286de66b90",
   "metadata": {},
   "source": [
    "### Different sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "379de911-ee09-4a78-b1c3-a9d0b96990dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:51.342143Z",
     "iopub.status.busy": "2024-04-14T18:27:51.341779Z",
     "iopub.status.idle": "2024-04-14T18:27:51.348405Z",
     "shell.execute_reply": "2024-04-14T18:27:51.347326Z",
     "shell.execute_reply.started": "2024-04-14T18:27:51.342108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "וש"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5579b40-c313-47e8-8ac3-f706b7999407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:52.816624Z",
     "iopub.status.busy": "2024-04-14T18:27:52.816237Z",
     "iopub.status.idle": "2024-04-14T18:27:52.823290Z",
     "shell.execute_reply": "2024-04-14T18:27:52.822296Z",
     "shell.execute_reply.started": "2024-04-14T18:27:52.816589Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "def create_baseline_by_pop(user_tensor):\n",
    "    baseline_tensor = torch.zeros_like(user_tensor)\n",
    "\n",
    "    for i in range(user_tensor.numel()):  # numel() gives the total number of elements in the tensor\n",
    "        random_sample = np.random.rand()\n",
    "        baseline_tensor[i] = 1 if random_sample < pop_array[i] else 0\n",
    "        \n",
    "    return baseline_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39641e22-a2a7-4a9f-bd64-2f325e1de4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:53.036857Z",
     "iopub.status.busy": "2024-04-14T18:27:53.035902Z",
     "iopub.status.idle": "2024-04-14T18:27:53.043313Z",
     "shell.execute_reply": "2024-04-14T18:27:53.042118Z",
     "shell.execute_reply.started": "2024-04-14T18:27:53.036816Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.random.seed(42) \n",
    "\n",
    "# def create_baseline_by_pop_only_1_items(user_tensor):\n",
    "#     baseline_tensor = torch.zeros_like(user_tensor)\n",
    "\n",
    "#     for i in range(user_tensor.numel()):\n",
    "#         if user_tensor[i] == 1:  # Only sample for entries where user_tensor is 1\n",
    "#             random_sample = np.random.rand()\n",
    "#             baseline_tensor[i] = 1 if random_sample < pop_array[i] else 0\n",
    "    \n",
    "#     return baseline_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c016090-2724-4781-9420-0fdb39b10e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:53.209044Z",
     "iopub.status.busy": "2024-04-14T18:27:53.208693Z",
     "iopub.status.idle": "2024-04-14T18:27:53.215503Z",
     "shell.execute_reply": "2024-04-14T18:27:53.214606Z",
     "shell.execute_reply.started": "2024-04-14T18:27:53.209010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "def create_baseline_random_items(user_tensor):\n",
    "    baseline_tensor = torch.zeros_like(user_tensor)\n",
    "\n",
    "    for i in range(user_tensor.numel()):  # numel() gives the total number of elements in the tensor\n",
    "        random_sample = np.random.rand()\n",
    "        baseline_tensor[i] = random_sample\n",
    "        \n",
    "    return baseline_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab3b5f50-ae88-4300-b206-206bca3c89e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:53.436062Z",
     "iopub.status.busy": "2024-04-14T18:27:53.435103Z",
     "iopub.status.idle": "2024-04-14T18:27:53.446544Z",
     "shell.execute_reply": "2024-04-14T18:27:53.445824Z",
     "shell.execute_reply.started": "2024-04-14T18:27:53.436021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_ip_mask(model, user_tensor, item_id, all_items_tensor, num_steps):\n",
    "    #baseline = torch.zeros_like(user_tensor)\n",
    "    if method == \"sample_random_user\" or method == \"sample_random_user_new\":  \n",
    "        baseline = create_baseline_random_user()\n",
    "    elif method == \"sample_items_by_pop\": \n",
    "        baseline = create_baseline_by_pop(user_tensor)\n",
    "    elif method == \"sample_only_1_items_by_pop\":\n",
    "        baseline = create_baseline_by_pop_only_1_items(user_tensor)\n",
    "    elif method == \"sample_random_items\":\n",
    "        baseline = create_baseline_random_items(user_tensor)\n",
    "    \n",
    "    interpolations = get_interpolated_values(baseline, user_tensor, num_steps)\n",
    "    \n",
    "    gradients = []\n",
    "    count = -1 \n",
    "    for i in interpolations: \n",
    "        count += 1\n",
    "        if count != 0:\n",
    "            i = i.to(device).detach()\n",
    "            i.requires_grad = True\n",
    "            grad_tensor = get_grads_wrt_to_user_tensor(model=model, user_tensor=i, all_items_tensor=all_items_tensor, item_id=item_id)\n",
    "            gradients.append(grad_tensor)\n",
    "\n",
    "    stacked_gradients = torch.stack(gradients, dim=0)\n",
    "    ip_explanation = torch.mean(stacked_gradients, dim=0)\n",
    "   \n",
    "    x_masked = user_tensor*ip_explanation \n",
    "    \n",
    "    item_sim_dict = {}\n",
    "    for i,j in enumerate(x_masked):\n",
    "        if j:\n",
    "            item_sim_dict[i]=x_masked[i] \n",
    "        \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76850d-4eec-4270-bca9-1993453ca0f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6000cfdb-6f63-4d6c-9d84-92b1a2d912f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:27:55.270827Z",
     "iopub.status.busy": "2024-04-14T18:27:55.270411Z",
     "iopub.status.idle": "2024-04-14T18:27:55.278768Z",
     "shell.execute_reply": "2024-04-14T18:27:55.277813Z",
     "shell.execute_reply.started": "2024-04-14T18:27:55.270790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender_model, all_items_tensor, user_id = None, mask_type = None):\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "    \n",
    "    \n",
    "    if mask_type == 'ip':\n",
    "        sim_items = find_ip_mask(model=recommender_model, user_tensor=user_tensor, item_id=item_id, all_items_tensor=all_items_tensor, num_steps=num_steps)\n",
    "        \n",
    "    else:\n",
    "        print (\"Wrong notebook!!\")\n",
    "        \n",
    "        \n",
    "    POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "    return POS_sim_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5874beaf-c840-4338-9161-e70e804621c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:28:00.894015Z",
     "iopub.status.busy": "2024-04-14T18:28:00.893589Z",
     "iopub.status.idle": "2024-04-14T18:28:00.919739Z",
     "shell.execute_reply": "2024-04-14T18:28:00.918996Z",
     "shell.execute_reply.started": "2024-04-14T18:28:00.893979Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender_model, expl_dict, **kw_dict):\n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "    \n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    rankA_at_1 = [0]*(len(bins))\n",
    "    rankA_at_5 = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    rankB = [0]*(len(bins))\n",
    "    NDCG = [0]*(len(bins))\n",
    "\n",
    "    \n",
    "    POS_sim_items = expl_dict\n",
    "    NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "    \n",
    "    total_items=0\n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "            \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked # remove the masked items from the user history\n",
    "\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "        \n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor, recommender_model, **kw_dict)\n",
    "        \n",
    "        if item_id in list(POS_ranked_list.keys()):\n",
    "            POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        else:\n",
    "            POS_index = num_items\n",
    "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, recommender_model, **kw_dict)+1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_run(POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_run(user_tensor-POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1+1-POS_index)/1)\n",
    "        rankA_at_5[i] = max(0, (5+1-POS_index)/5)\n",
    "        rankA_at_10[i] = max(0, (10+1-POS_index)/10)\n",
    "        rankA_at_20[i] = max(0, (20+1-POS_index)/20)\n",
    "        rankA_at_50[i] = max(0, (50+1-POS_index)/50)\n",
    "        rankA_at_100[i] = max(0, (100+1-POS_index)/100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1/POS_index\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG[i]= get_ndcg(list(POS_ranked_list.keys()),item_id, **kw_dict)\n",
    "        \n",
    "    res = [DEL, INS, rankB, NDCG, POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100,  NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100,  rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e11af-1c1c-4f47-baa9-b23e7696ed04",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b56d849-8a5a-4593-b332-7f615a9ff584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:30:40.811190Z",
     "iopub.status.busy": "2024-04-14T18:30:40.810758Z",
     "iopub.status.idle": "2024-04-14T18:31:31.249650Z",
     "shell.execute_reply": "2024-04-14T18:31:31.248641Z",
     "shell.execute_reply.started": "2024-04-14T18:30:40.811154Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.defs.recommenders_architecture:37: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/aten/src/ATen/native/TensorShape.cpp:2981.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for data_name in data_names:\n",
    "    \n",
    "    DP_DIR = Path(\"processed_data\", data_name)\n",
    "    files_path = Path(export_dir, DP_DIR)\n",
    "\n",
    "    num_users = num_users_dict[data_name] \n",
    "    num_items = num_items_dict[data_name] \n",
    "    demographic = demographic_dict[data_name]\n",
    "    if demographic:\n",
    "        num_features = features_dict[data_name]\n",
    "    else:\n",
    "        num_features = num_items_dict[data_name]\n",
    "    with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "        pop_dict = pickle.load(f) \n",
    "    pop_array = np.zeros(len(pop_dict))\n",
    "    for key, value in pop_dict.items():\n",
    "        pop_array[key] = value\n",
    "\n",
    "    \n",
    "    # Data \n",
    "    train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "    test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "    static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "    \n",
    "    train_array = train_data.to_numpy()\n",
    "    test_array = test_data.to_numpy()\n",
    "    items_array = np.eye(num_items)\n",
    "    all_items_tensor = torch.Tensor(items_array).to(device)\n",
    "    test_array = static_test_data.iloc[:,:-2].to_numpy()\n",
    "\n",
    "    \n",
    "    for recommender_name in recommender_names:\n",
    "        output_type = output_type_dict[recommender_name]\n",
    "        hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "        \n",
    "        recommender_path = recommender_path_dict[(data_name,recommender_name)]\n",
    "\n",
    "        kw_dict = {'device':device,\n",
    "                  'num_items': num_items,\n",
    "                  'demographic':demographic,\n",
    "                  'num_features':num_features,\n",
    "                  'pop_array':pop_array,\n",
    "                  'all_items_tensor':all_items_tensor,\n",
    "                  'static_test_data':static_test_data,\n",
    "                  'items_array':items_array,\n",
    "                  'output_type':output_type,\n",
    "                  'recommender_name':recommender_name}\n",
    "\n",
    "\n",
    "        recommender = load_recommender()\n",
    "\n",
    "        file_mode = 'a' if os.path.exists(new_file_name) else 'w'\n",
    "        with open(new_file_name, file_mode) as file:\n",
    "            file.write(f' ============ This stats are for {data_name} dataset ============\\n')\n",
    "            file.write(f' ============ & for the recommender {recommender_name} ============\\n')\n",
    "            \n",
    "        file_mode = 'a'\n",
    "        \n",
    "        for expl_name in expl_names_list:\n",
    "            if expl_name == \"ip\":\n",
    "                with open(new_file_name, file_mode) as file:\n",
    "                    file.write(f' ============ Start explaining by {expl_name} ============\\n')\n",
    "                    now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    file.write(f' ============ Start time {now} ============\\n')\n",
    "\n",
    "                ip_path = Path(files_path, \"ip\", recommender_name, method)\n",
    "\n",
    "                recommender.eval()\n",
    "                users_DEL = {}\n",
    "                users_INS = {}\n",
    "                reciprocal =  {}\n",
    "                NDCG = {}\n",
    "                POS_at_1 = {}\n",
    "                POS_at_5 = {}\n",
    "                POS_at_10 = {}\n",
    "                POS_at_20 = {}\n",
    "                POS_at_50 = {}\n",
    "                POS_at_100 = {}\n",
    "                NEG_at_1 = {}\n",
    "                NEG_at_5 = {}\n",
    "                NEG_at_10 = {}\n",
    "                NEG_at_20 = {}\n",
    "                NEG_at_50 = {}\n",
    "                NEG_at_100 = {}\n",
    "                rank_at_1 = {}\n",
    "                rank_at_5 = {}\n",
    "                rank_at_10 = {}\n",
    "                rank_at_20 = {}\n",
    "                rank_at_50 = {}\n",
    "                rank_at_100 = {}\n",
    "\n",
    "                # Calculate the number of empty lists needed\n",
    "                num_lists_needed = len(list_of_nums)\n",
    "\n",
    "                all_dicts = [\n",
    "                    users_DEL, users_INS, reciprocal, NDCG, POS_at_1, POS_at_5, POS_at_10, \n",
    "                    POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, \n",
    "                    NEG_at_20, NEG_at_50, NEG_at_100, rank_at_1, rank_at_5, rank_at_10, \n",
    "                    rank_at_20, rank_at_50, rank_at_100]\n",
    "\n",
    "                # Append the required number of empty lists to each list in all_lists\n",
    "                for each_dict in all_dicts:\n",
    "                    for n in range(num_lists_needed):\n",
    "                        each_dict[f\"num_of_users_{list_of_nums[n]}\"] = []\n",
    "\n",
    "                num_of_bins=10\n",
    "\n",
    "                for i in range(test_array.shape[0]):\n",
    "                #for i in range(3):\n",
    "                    if i%50 == 0:\n",
    "                        print(i)\n",
    "                    if i % 5 == 0:\n",
    "                        with open(new_file_name, file_mode) as file:\n",
    "                            now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            file.write(f' ============ User number {i} has started at {now} \\n')\n",
    "                    user_vector = test_array[i]\n",
    "                    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "                    user_id = int(test_data.index[i])\n",
    "                    item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "                    item_vector =  items_array[item_id]\n",
    "                    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "                    user_vector[item_id] = 0\n",
    "                    user_tensor[item_id] = 0\n",
    "\n",
    "                    user_DEL = []\n",
    "                    user_INS = []\n",
    "                    user_reciprocal = []\n",
    "                    user_NDCG = []\n",
    "                    user_POS_at_1 = []\n",
    "                    user_POS_at_5 = []\n",
    "                    user_POS_at_10 = []\n",
    "                    user_POS_at_20 = []\n",
    "                    user_POS_at_50 = []\n",
    "                    user_POS_at_100 = []\n",
    "                    user_NEG_at_1 = []\n",
    "                    user_NEG_at_5 = []\n",
    "                    user_NEG_at_10 = []\n",
    "                    user_NEG_at_20 = []\n",
    "                    user_NEG_at_50 = []\n",
    "                    user_NEG_at_100 = []\n",
    "                    user_rank_at_1 = []\n",
    "                    user_rank_at_5 = []\n",
    "                    user_rank_at_10 = []\n",
    "                    user_rank_at_20 = []\n",
    "                    user_rank_at_50 = []\n",
    "                    user_rank_at_100 = []\n",
    "\n",
    "                    for j in range(1, num_of_random_users + 1):\n",
    "                        user_expl_dict = {}\n",
    "                        user_expl = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, all_items_tensor=kw_dict['all_items_tensor'], mask_type= 'ip')\n",
    "                        user_expl_dict[user_id] = user_expl\n",
    "                        num = str(j)\n",
    "\n",
    "                        new_dict_path = Path(ip_path, num, f'ip_expl_dict_user_{i}.pkl')\n",
    "                        new_dict_path.parent.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "                        with open(new_dict_path, 'wb') as handle:\n",
    "                            pickle.dump(user_expl_dict, handle)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            res = single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender, user_expl, **kw_dict)\n",
    "\n",
    "                            user_DEL.append(np.mean(res[0]))\n",
    "                            user_INS.append(np.mean(res[1]))\n",
    "                            user_reciprocal.append(np.mean(res[2]))\n",
    "                            user_NDCG.append(np.mean(res[3]))\n",
    "                            user_POS_at_1.append(np.mean(res[4]))\n",
    "                            user_POS_at_5.append(np.mean(res[5]))\n",
    "                            user_POS_at_10.append(np.mean(res[6]))\n",
    "                            user_POS_at_20.append(np.mean(res[7]))\n",
    "                            user_POS_at_50.append(np.mean(res[8]))\n",
    "                            user_POS_at_100.append(np.mean(res[9]))\n",
    "                            user_NEG_at_1.append(np.mean(res[10]))\n",
    "                            user_NEG_at_5.append(np.mean(res[11]))\n",
    "                            user_NEG_at_10.append(np.mean(res[12]))\n",
    "                            user_NEG_at_20.append(np.mean(res[13]))\n",
    "                            user_NEG_at_50.append(np.mean(res[14]))\n",
    "                            user_NEG_at_100.append(np.mean(res[15]))\n",
    "                            user_rank_at_1.append(np.mean(res[16]))\n",
    "                            user_rank_at_5.append(np.mean(res[17]))\n",
    "                            user_rank_at_10.append(np.mean(res[18]))\n",
    "                            user_rank_at_20.append(np.mean(res[19]))\n",
    "                            user_rank_at_50.append(np.mean(res[20]))\n",
    "                            user_rank_at_100.append(np.mean(res[21]))\n",
    "\n",
    "                            if j in list_of_nums:\n",
    "                                key_name = f\"num_of_users_{j}\"\n",
    "\n",
    "                                users_DEL[key_name].append(min(user_DEL))\n",
    "                                users_INS[key_name].append(max(user_INS))\n",
    "                                reciprocal[key_name].append(min(user_reciprocal))\n",
    "                                NDCG[key_name].append(min(user_NDCG))\n",
    "                                POS_at_1[key_name].append(min(user_POS_at_1))\n",
    "                                POS_at_5[key_name].append(min(user_POS_at_5))\n",
    "                                POS_at_10[key_name].append(min(user_POS_at_10))\n",
    "                                POS_at_20[key_name].append(min(user_POS_at_20))\n",
    "                                POS_at_50[key_name].append(min(user_POS_at_50))\n",
    "                                POS_at_100[key_name].append(min(user_POS_at_100))\n",
    "                                NEG_at_1[key_name].append(max(user_NEG_at_1))\n",
    "                                NEG_at_5[key_name].append(max(user_NEG_at_5))\n",
    "                                NEG_at_10[key_name].append(max(user_NEG_at_10))\n",
    "                                NEG_at_20[key_name].append(max(user_NEG_at_20))\n",
    "                                NEG_at_50[key_name].append(max(user_NEG_at_50))\n",
    "                                NEG_at_100[key_name].append(max(user_NEG_at_100))\n",
    "                                rank_at_1[key_name].append(min(user_rank_at_1))\n",
    "                                rank_at_5[key_name].append(min(user_rank_at_5))\n",
    "                                rank_at_10[key_name].append(min(user_rank_at_10))\n",
    "                                rank_at_20[key_name].append(min(user_rank_at_20))\n",
    "                                rank_at_50[key_name].append(min(user_rank_at_50))\n",
    "                                rank_at_100[key_name].append(min(user_rank_at_100))\n",
    "\n",
    "        with open(new_file_name, file_mode) as file:                        \n",
    "            final = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            file.write(f' \\n')\n",
    "            file.write(f' ============ All users are done at {final} ============\\n')\n",
    "\n",
    "            results_path = Path(files_path, \"ip\", recommender_name, method, \"results\")      \n",
    "            results_path.mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "            ip_final_statistics = {}\n",
    "            for k in range(num_lists_needed):\n",
    "                spot = f\"num_of_users_{list_of_nums[k]}\"\n",
    "                ip_final_statistics[f\"{list_of_nums[k]}_random\"] = [np.mean(users_DEL[spot]), np.mean(users_INS[spot]), np.mean(reciprocal[spot]), np.mean(NDCG[spot]), np.mean(POS_at_1[spot]), np.mean(NEG_at_1[spot]), np.mean(rank_at_1[spot]), np.mean(POS_at_5[spot]), np.mean(NEG_at_5[spot]), np.mean(rank_at_5[spot]), np.mean(POS_at_10[spot]), np.mean(NEG_at_10[spot]), np.mean(rank_at_10[spot]), np.mean(POS_at_20[spot]), np.mean(NEG_at_20[spot]), np.mean(rank_at_20[spot]), np.mean(POS_at_50[spot]), np.mean(NEG_at_50[spot]), np.mean(rank_at_50[spot]), np.mean(POS_at_100[spot]), np.mean(NEG_at_100[spot]), np.mean(rank_at_100[spot])]\n",
    "                file.write(f' ============ Results for {list_of_nums[k]} random users: ============ \\n')\n",
    "                file.write(f'{ip_final_statistics[f\"{list_of_nums[k]}_random\"]} \\n')\n",
    "\n",
    "            # with open(Path(files_path,f'{recommender_name}_ip_all_statistics.pkl'), 'wb') as handle:\n",
    "            #     pickle.dump(ip_all_statistics, handle)\n",
    "\n",
    "\n",
    "            with open(Path(results_path,f'{method}_final_statistics.pkl'), 'wb') as handle:\n",
    "                pickle.dump(ip_final_statistics, handle)\n",
    "\n",
    "            #file.write(f\"{users_DEL/a}, {users_INS/a}, {reciprocal/a}, {NDCG/a}, {POS_at_1/a}, {NEG_at_1/a}, {rank_at_1/a}, {POS_at_5/a}, {NEG_at_5/a}, {rank_at_5/a}, {POS_at_10/a}, {NEG_at_10/a}, {rank_at_10/a}, {POS_at_20/a}, {NEG_at_20/a}, {rank_at_20/a}, {POS_at_50/a}, {NEG_at_50/a}, {rank_at_50/a}, {POS_at_100/a}, {NEG_at_100/a}, {rank_at_100/a}\\n\")\n",
    "            file.write(f\"All Saved \\n\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7cd73a3-18dd-4218-9e6e-df67dbbf6778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T18:31:31.251173Z",
     "iopub.status.busy": "2024-04-14T18:31:31.250957Z",
     "iopub.status.idle": "2024-04-14T18:31:31.255865Z",
     "shell.execute_reply": "2024-04-14T18:31:31.255041Z",
     "shell.execute_reply.started": "2024-04-14T18:31:31.251151Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c1cfa-f421-46a8-9871-017d291f7563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3af60d-f6d4-4f99-90dc-82cd9ee13d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010c591-d1f2-48b1-9a7f-0909bd06c239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20c9d7-7618-4398-978d-e4bc8a4f90a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
